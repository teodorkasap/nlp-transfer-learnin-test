{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","from transformers import AdamW\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from IPython.core.interactiveshell import InteractiveShell\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","import time\n","\n","# specify GPU\n","device = torch.device(\"cuda\")\n","\n","# This is for multiple print statements per cell\n","InteractiveShell.ast_node_interactivity = \"all\"\n",""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text\n","0  negative                       oh no its fading away again \n","1  positive  bunnylake will kill me but i cant stop listeni...\n","2  negative  last day in cali  partyin for the last time wi...\n","3  negative                     is having a major soar throat \n","4  positive                       my last day as 12 years old "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>oh no its fading away again</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>bunnylake will kill me but i cant stop listeni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>last day in cali  partyin for the last time wi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>negative</td>\n      <td>is having a major soar throat</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>my last day as 12 years old</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":2}],"source":["df = pd.read_csv(\"pre-cleaned_consolidated_tweet_data.csv\", sep='\\t')\n","df.head()\n",""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text  label\n","0  negative                       oh no its fading away again       0\n","1  positive  bunnylake will kill me but i cant stop listeni...      1\n","2  negative  last day in cali  partyin for the last time wi...      0\n","3  negative                     is having a major soar throat       0\n","4  positive                       my last day as 12 years old       1"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>oh no its fading away again</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>bunnylake will kill me but i cant stop listeni...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>last day in cali  partyin for the last time wi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>negative</td>\n      <td>is having a major soar throat</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>my last day as 12 years old</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":3}],"source":["df['label'] = df.apply(lambda x: 1 if x['sentiment'] == 'positive' else 0,axis=1)\n","df.head()\n",""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   label                                               text\n","0      0                       oh no its fading away again \n","1      1  bunnylake will kill me but i cant stop listeni...\n","2      0  last day in cali  partyin for the last time wi...\n","3      0                     is having a major soar throat \n","4      1                       my last day as 12 years old "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>oh no its fading away again</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>bunnylake will kill me but i cant stop listeni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>last day in cali  partyin for the last time wi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>is having a major soar throat</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>my last day as 12 years old</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":4}],"source":["df = df[['label', 'text']]\n","df.head()\n","\n",""]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","from transformers import AdamW\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from IPython.core.interactiveshell import InteractiveShell\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","import time\n","\n","# specify GPU\n","device = torch.device(\"cuda\")\n","\n","# This is for multiple print statements per cell\n","InteractiveShell.ast_node_interactivity = \"all\"\n",""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   label                                               text\n","0      0  Go until jurong point, crazy.. Available only ...\n","1      0                      Ok lar... Joking wif u oni...\n","2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n","3      0  U dun say so early hor... U c already then say...\n","4      0  Nah I don't think he goes to usf, he lives aro..."],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":6}],"source":["df = pd.read_csv(\"spamdata_v2.txt\")\n","df.head()\n",""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","from transformers import AdamW\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from IPython.core.interactiveshell import InteractiveShell\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","import time\n","\n","# specify GPU\n","device = torch.device(\"cuda\")\n","\n","# This is for multiple print statements per cell\n","InteractiveShell.ast_node_interactivity = \"all\"\n",""]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   label                                               text\n","0      0  Go until jurong point, crazy.. Available only ...\n","1      0                      Ok lar... Joking wif u oni...\n","2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n","3      0  U dun say so early hor... U c already then say...\n","4      0  Nah I don't think he goes to usf, he lives aro..."],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":8}],"source":["df = pd.read_csv(\"spamdata_v2.txt\")\n","df.head()\n",""]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["train_text, temp_text, train_labels, temp_labels = \\\n","    train_test_split(df['text'], df['label'], random_state=2018,\n","                     test_size=0.3, stratify=df['label'])\n","\n","\n","val_text, test_text, val_labels, test_labels = \\\n","    train_test_split(temp_text, temp_labels, random_state=2018,\n","                     test_size=0.5, stratify=temp_labels)\n","\n",""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["bert = AutoModel.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",""]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["sent_id = tokenizer.batch_encode_plus(\n","    text, padding=True, return_token_type_ids=False)\n",""]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}],"source":["print(sent_id)\n",""]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AxesSubplot:>"]},"metadata":{},"execution_count":15},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 383.137066 248.518125\" width=\"383.137066pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-10-27T22:00:36.669050</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 248.518125 \nL 383.137066 248.518125 \nL 383.137066 0 \nL -0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 39.65 224.64 \nL 374.45 224.64 \nL 374.45 7.2 \nL 39.65 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 54.868182 224.64 \nL 65.013636 224.64 \nL 65.013636 94.980494 \nL 54.868182 94.980494 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 65.013636 224.64 \nL 75.159091 224.64 \nL 75.159091 17.554286 \nL 65.013636 17.554286 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 75.159091 224.64 \nL 85.304545 224.64 \nL 85.304545 137.472534 \nL 75.159091 137.472534 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 85.304545 224.64 \nL 95.45 224.64 \nL 95.45 143.350879 \nL 85.304545 143.350879 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 95.45 224.64 \nL 105.595455 224.64 \nL 105.595455 132.601905 \nL 95.45 132.601905 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 105.595455 224.64 \nL 115.740909 224.64 \nL 115.740909 188.698116 \nL 105.595455 188.698116 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 115.740909 224.64 \nL 125.886364 224.64 \nL 125.886364 214.226931 \nL 115.740909 214.226931 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 125.886364 224.64 \nL 136.031818 224.64 \nL 136.031818 221.112993 \nL 125.886364 221.112993 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 136.031818 224.64 \nL 146.177273 224.64 \nL 146.177273 223.632284 \nL 136.031818 223.632284 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 146.177273 224.64 \nL 156.322727 224.64 \nL 156.322727 223.464331 \nL 146.177273 223.464331 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 156.322727 224.64 \nL 166.468182 224.64 \nL 166.468182 222.288662 \nL 156.322727 222.288662 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 166.468182 224.64 \nL 176.613636 224.64 \nL 176.613636 224.472047 \nL 166.468182 224.472047 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 176.613636 224.64 \nL 186.759091 224.64 \nL 186.759091 223.632284 \nL 176.613636 223.632284 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 186.759091 224.64 \nL 196.904545 224.64 \nL 196.904545 224.136142 \nL 186.759091 224.136142 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 196.904545 224.64 \nL 207.05 224.64 \nL 207.05 224.64 \nL 196.904545 224.64 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 207.05 224.64 \nL 217.195455 224.64 \nL 217.195455 224.304095 \nL 207.05 224.304095 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 217.195455 224.64 \nL 227.340909 224.64 \nL 227.340909 224.136142 \nL 217.195455 224.136142 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 227.340909 224.64 \nL 237.486364 224.64 \nL 237.486364 224.64 \nL 227.340909 224.64 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 237.486364 224.64 \nL 247.631818 224.64 \nL 247.631818 224.472047 \nL 237.486364 224.472047 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 247.631818 224.64 \nL 257.777273 224.64 \nL 257.777273 224.472047 \nL 247.631818 224.472047 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 257.777273 224.64 \nL 267.922727 224.64 \nL 267.922727 224.64 \nL 257.777273 224.64 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 267.922727 224.64 \nL 278.068182 224.64 \nL 278.068182 224.472047 \nL 267.922727 224.472047 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 278.068182 224.64 \nL 288.213636 224.64 \nL 288.213636 224.64 \nL 278.068182 224.64 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 288.213636 224.64 \nL 298.359091 224.64 \nL 298.359091 224.64 \nL 288.213636 224.64 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 298.359091 224.64 \nL 308.504545 224.64 \nL 308.504545 224.64 \nL 298.359091 224.64 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 308.504545 224.64 \nL 318.65 224.64 \nL 318.65 224.64 \nL 308.504545 224.64 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 318.65 224.64 \nL 328.795455 224.64 \nL 328.795455 224.64 \nL 318.65 224.64 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 328.795455 224.64 \nL 338.940909 224.64 \nL 338.940909 224.64 \nL 328.795455 224.64 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 338.940909 224.64 \nL 349.086364 224.64 \nL 349.086364 224.472047 \nL 338.940909 224.472047 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path clip-path=\"url(#p160b4293b8)\" d=\"M 349.086364 224.64 \nL 359.231818 224.64 \nL 359.231818 224.472047 \nL 349.086364 224.472047 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p160b4293b8)\" d=\"M 53.077807 224.64 \nL 53.077807 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m9dadde0745\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.077807\" xlink:href=\"#m9dadde0745\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(49.896557 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p160b4293b8)\" d=\"M 97.837166 224.64 \nL 97.837166 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"97.837166\" xlink:href=\"#m9dadde0745\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 25 -->\n      <g transform=\"translate(91.474666 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p160b4293b8)\" d=\"M 142.596524 224.64 \nL 142.596524 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"142.596524\" xlink:href=\"#m9dadde0745\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 50 -->\n      <g transform=\"translate(136.234024 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p160b4293b8)\" d=\"M 187.355882 224.64 \nL 187.355882 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"187.355882\" xlink:href=\"#m9dadde0745\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 75 -->\n      <g transform=\"translate(180.993382 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p160b4293b8)\" d=\"M 232.115241 224.64 \nL 232.115241 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.115241\" xlink:href=\"#m9dadde0745\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 100 -->\n      <g transform=\"translate(222.571491 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p160b4293b8)\" d=\"M 276.874599 224.64 \nL 276.874599 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"276.874599\" xlink:href=\"#m9dadde0745\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 125 -->\n      <g transform=\"translate(267.330849 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p160b4293b8)\" d=\"M 321.633957 224.64 \nL 321.633957 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"321.633957\" xlink:href=\"#m9dadde0745\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 150 -->\n      <g transform=\"translate(312.090207 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p160b4293b8)\" d=\"M 366.393316 224.64 \nL 366.393316 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"366.393316\" xlink:href=\"#m9dadde0745\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 175 -->\n      <g transform=\"translate(356.849566 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p160b4293b8)\" d=\"M 39.65 224.64 \nL 374.45 224.64 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m388538d81f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m388538d81f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0 -->\n      <g transform=\"translate(26.2875 228.439219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p160b4293b8)\" d=\"M 39.65 191.049454 \nL 374.45 191.049454 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m388538d81f\" y=\"191.049454\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 200 -->\n      <g transform=\"translate(13.5625 194.848673)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p160b4293b8)\" d=\"M 39.65 157.458909 \nL 374.45 157.458909 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m388538d81f\" y=\"157.458909\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 400 -->\n      <g transform=\"translate(13.5625 161.258127)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p160b4293b8)\" d=\"M 39.65 123.868363 \nL 374.45 123.868363 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m388538d81f\" y=\"123.868363\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 600 -->\n      <g transform=\"translate(13.5625 127.667582)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p160b4293b8)\" d=\"M 39.65 90.277817 \nL 374.45 90.277817 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m388538d81f\" y=\"90.277817\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 800 -->\n      <g transform=\"translate(13.5625 94.077036)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p160b4293b8)\" d=\"M 39.65 56.687271 \nL 374.45 56.687271 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m388538d81f\" y=\"56.687271\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1000 -->\n      <g transform=\"translate(7.2 60.48649)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p160b4293b8)\" d=\"M 39.65 23.096726 \nL 374.45 23.096726 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m388538d81f\" y=\"23.096726\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 1200 -->\n      <g transform=\"translate(7.2 26.895945)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 39.65 224.64 \nL 39.65 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 374.45 224.64 \nL 374.45 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 39.65 224.64 \nL 374.45 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 39.65 7.2 \nL 374.45 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p160b4293b8\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"39.65\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["seq_len = [len(i.split()) for i in train_text]\n","\n","pd.Series(seq_len).hist(bins=30)\n",""]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","from transformers import AdamW\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from IPython.core.interactiveshell import InteractiveShell\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","import time\n","\n","# specify GPU\n","device = torch.device(\"cuda\")\n","\n","# This is for multiple print statements per cell\n","InteractiveShell.ast_node_interactivity = \"all\"\n",""]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text\n","0  negative                       oh no its fading away again \n","1  positive  bunnylake will kill me but i cant stop listeni...\n","2  negative  last day in cali  partyin for the last time wi...\n","3  negative                     is having a major soar throat \n","4  positive                       my last day as 12 years old "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>oh no its fading away again</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>bunnylake will kill me but i cant stop listeni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>last day in cali  partyin for the last time wi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>negative</td>\n      <td>is having a major soar throat</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>my last day as 12 years old</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":17}],"source":["df = pd.read_csv(\"pre-cleaned_consolidated_tweet_data.csv\", sep='\\t')\n","df.head()\n",""]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text  label\n","0  negative                       oh no its fading away again       0\n","1  positive  bunnylake will kill me but i cant stop listeni...      1\n","2  negative  last day in cali  partyin for the last time wi...      0\n","3  negative                     is having a major soar throat       0\n","4  positive                       my last day as 12 years old       1"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>oh no its fading away again</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>bunnylake will kill me but i cant stop listeni...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>last day in cali  partyin for the last time wi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>negative</td>\n      <td>is having a major soar throat</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>my last day as 12 years old</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":18}],"source":["df['label'] = df.apply(lambda x: 1 if x['sentiment']\n","                       == 'positive' else 0, axis=1)\n","df.head()\n",""]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   label                                               text\n","0      0                       oh no its fading away again \n","1      1  bunnylake will kill me but i cant stop listeni...\n","2      0  last day in cali  partyin for the last time wi...\n","3      0                     is having a major soar throat \n","4      1                       my last day as 12 years old "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>oh no its fading away again</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>bunnylake will kill me but i cant stop listeni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>last day in cali  partyin for the last time wi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>is having a major soar throat</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>my last day as 12 years old</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":19}],"source":["df = df[['label', 'text']]\n","df.head()\n","\n",""]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["train_text, temp_text, train_labels, temp_labels = \\\n","    train_test_split(df['text'], df['label'], random_state=2018,\n","                     test_size=0.3, stratify=df['label'])\n","\n","\n","val_text, test_text, val_labels, test_labels = \\\n","    train_test_split(temp_text, temp_labels, random_state=2018,\n","                     test_size=0.5, stratify=temp_labels)\n","\n",""]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["bert = AutoModel.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",""]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["sent_id = tokenizer.batch_encode_plus(\n","    text, padding=True, return_token_type_ids=False)\n",""]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}],"source":["print(sent_id)\n",""]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AxesSubplot:>"]},"metadata":{},"execution_count":26},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 394.375 248.518125\" width=\"394.375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-10-27T22:01:07.739390</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 394.375 248.518125 \nL 394.375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 52.375 224.64 \nL 387.175 224.64 \nL 387.175 7.2 \nL 52.375 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 67.593182 224.64 \nL 77.738636 224.64 \nL 77.738636 184.822775 \nL 67.593182 184.822775 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 77.738636 224.64 \nL 87.884091 224.64 \nL 87.884091 159.810294 \nL 77.738636 159.810294 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 87.884091 224.64 \nL 98.029545 224.64 \nL 98.029545 140.747593 \nL 87.884091 140.747593 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 98.029545 224.64 \nL 108.175 224.64 \nL 108.175 18.392453 \nL 98.029545 18.392453 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 108.175 224.64 \nL 118.320455 224.64 \nL 118.320455 115.626685 \nL 108.175 115.626685 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 118.320455 224.64 \nL 128.465909 224.64 \nL 128.465909 116.277254 \nL 118.320455 116.277254 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 128.465909 224.64 \nL 138.611364 224.64 \nL 138.611364 17.554286 \nL 128.465909 17.554286 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 138.611364 224.64 \nL 148.756818 224.64 \nL 148.756818 127.447074 \nL 138.611364 127.447074 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 148.756818 224.64 \nL 158.902273 224.64 \nL 158.902273 132.393463 \nL 148.756818 132.393463 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 158.902273 224.64 \nL 169.047727 224.64 \nL 169.047727 55.612566 \nL 158.902273 55.612566 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 169.047727 224.64 \nL 179.193182 224.64 \nL 179.193182 147.761001 \nL 169.047727 147.761001 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 179.193182 224.64 \nL 189.338636 224.64 \nL 189.338636 152.075753 \nL 179.193182 152.075753 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 189.338636 224.64 \nL 199.484091 224.64 \nL 199.484091 90.765659 \nL 189.338636 90.765659 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 199.484091 224.64 \nL 209.629545 224.64 \nL 209.629545 161.74479 \nL 199.484091 161.74479 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 209.629545 224.64 \nL 219.775 224.64 \nL 219.775 161.806749 \nL 209.629545 161.806749 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 219.775 224.64 \nL 229.920455 224.64 \nL 229.920455 107.413683 \nL 219.775 107.413683 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 229.920455 224.64 \nL 240.065909 224.64 \nL 240.065909 169.145441 \nL 229.920455 169.145441 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 240.065909 224.64 \nL 250.211364 224.64 \nL 250.211364 173.305296 \nL 240.065909 173.305296 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 250.211364 224.64 \nL 260.356818 224.64 \nL 260.356818 145.83339 \nL 250.211364 145.83339 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 260.356818 224.64 \nL 270.502273 224.64 \nL 270.502273 200.923494 \nL 260.356818 200.923494 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 270.502273 224.64 \nL 280.647727 224.64 \nL 280.647727 210.052111 \nL 270.502273 210.052111 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 280.647727 224.64 \nL 290.793182 224.64 \nL 290.793182 213.361752 \nL 280.647727 213.361752 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 290.793182 224.64 \nL 300.938636 224.64 \nL 300.938636 223.092748 \nL 290.793182 223.092748 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 300.938636 224.64 \nL 311.084091 224.64 \nL 311.084091 224.118512 \nL 300.938636 224.118512 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 311.084091 224.64 \nL 321.229545 224.64 \nL 321.229545 224.424865 \nL 311.084091 224.424865 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 321.229545 224.64 \nL 331.375 224.64 \nL 331.375 224.626231 \nL 321.229545 224.626231 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 331.375 224.64 \nL 341.520455 224.64 \nL 341.520455 224.634837 \nL 331.375 224.634837 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 341.520455 224.64 \nL 351.665909 224.64 \nL 351.665909 224.64 \nL 341.520455 224.64 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 351.665909 224.64 \nL 361.811364 224.64 \nL 361.811364 224.636558 \nL 351.665909 224.636558 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 361.811364 224.64 \nL 371.956818 224.64 \nL 371.956818 224.638279 \nL 361.811364 224.638279 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 59.984091 224.64 \nL 59.984091 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m0b2acb3cef\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"59.984091\" xlink:href=\"#m0b2acb3cef\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(56.802841 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 98.029545 224.64 \nL 98.029545 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"98.029545\" xlink:href=\"#m0b2acb3cef\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(94.848295 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 136.075 224.64 \nL 136.075 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"136.075\" xlink:href=\"#m0b2acb3cef\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(129.7125 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 174.120455 224.64 \nL 174.120455 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"174.120455\" xlink:href=\"#m0b2acb3cef\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(167.757955 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 212.165909 224.64 \nL 212.165909 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"212.165909\" xlink:href=\"#m0b2acb3cef\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(205.803409 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 250.211364 224.64 \nL 250.211364 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"250.211364\" xlink:href=\"#m0b2acb3cef\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(243.848864 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 288.256818 224.64 \nL 288.256818 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"288.256818\" xlink:href=\"#m0b2acb3cef\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <g transform=\"translate(281.894318 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 326.302273 224.64 \nL 326.302273 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"326.302273\" xlink:href=\"#m0b2acb3cef\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 35 -->\n      <g transform=\"translate(319.939773 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 364.347727 224.64 \nL 364.347727 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"364.347727\" xlink:href=\"#m0b2acb3cef\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 40 -->\n      <g transform=\"translate(357.985227 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 52.375 224.64 \nL 387.175 224.64 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mb228c8b22d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mb228c8b22d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0 -->\n      <g transform=\"translate(39.0125 228.439219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 52.375 190.218366 \nL 387.175 190.218366 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mb228c8b22d\" y=\"190.218366\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20000 -->\n      <g transform=\"translate(13.5625 194.017585)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 52.375 155.796732 \nL 387.175 155.796732 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mb228c8b22d\" y=\"155.796732\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 40000 -->\n      <g transform=\"translate(13.5625 159.59595)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 52.375 121.375098 \nL 387.175 121.375098 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mb228c8b22d\" y=\"121.375098\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 60000 -->\n      <g transform=\"translate(13.5625 125.174316)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 52.375 86.953463 \nL 387.175 86.953463 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mb228c8b22d\" y=\"86.953463\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 80000 -->\n      <g transform=\"translate(13.5625 90.752682)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 52.375 52.531829 \nL 387.175 52.531829 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mb228c8b22d\" y=\"52.531829\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 100000 -->\n      <g transform=\"translate(7.2 56.331048)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#pee4acfd8ee)\" d=\"M 52.375 18.110195 \nL 387.175 18.110195 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mb228c8b22d\" y=\"18.110195\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 120000 -->\n      <g transform=\"translate(7.2 21.909414)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 52.375 224.64 \nL 52.375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 387.175 224.64 \nL 387.175 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 52.375 224.64 \nL 387.175 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 52.375 7.2 \nL 387.175 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pee4acfd8ee\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"52.375\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["seq_len = [len(i.split()) for i in train_text]\n","\n","pd.Series(seq_len).hist(bins=30)\n",""]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","from transformers import AdamW\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from IPython.core.interactiveshell import InteractiveShell\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","import time\n","\n","# specify GPU\n","device = torch.device(\"cuda\")\n","\n","# This is for multiple print statements per cell\n","InteractiveShell.ast_node_interactivity = \"all\"\n",""]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text\n","0  negative                       oh no its fading away again \n","1  positive  bunnylake will kill me but i cant stop listeni...\n","2  negative  last day in cali  partyin for the last time wi...\n","3  negative                     is having a major soar throat \n","4  positive                       my last day as 12 years old "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>oh no its fading away again</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>bunnylake will kill me but i cant stop listeni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>last day in cali  partyin for the last time wi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>negative</td>\n      <td>is having a major soar throat</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>my last day as 12 years old</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":28}],"source":["df = pd.read_csv(\"pre-cleaned_consolidated_tweet_data.csv\", sep='\\t')\n","df.head()\n",""]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["frac = 0.2\n","# sample and shuffle the dataset according to the fraction choise in the line above\n","df = df.sample(frac=frac).reset_index(drop=True)\n","\n",""]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text  label\n","0  positive  suggesting potential conference keynote speake...      1\n","1  negative  jeffarchuleta glad the new members are meshing...      0\n","2  negative          nicramirez my wisdom tooth is growing in       0\n","3  negative                                      i miss mcfly       0\n","4  positive                  kirstyhawkshaw love the new song       1"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>positive</td>\n      <td>suggesting potential conference keynote speake...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>negative</td>\n      <td>jeffarchuleta glad the new members are meshing...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>nicramirez my wisdom tooth is growing in</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>negative</td>\n      <td>i miss mcfly</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>kirstyhawkshaw love the new song</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":30}],"source":["df['label'] = df.apply(lambda x: 1 if x['sentiment']\n","                       == 'positive' else 0, axis=1)\n","df.head()\n",""]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   label                                               text\n","0      1  suggesting potential conference keynote speake...\n","1      0  jeffarchuleta glad the new members are meshing...\n","2      0          nicramirez my wisdom tooth is growing in \n","3      0                                      i miss mcfly \n","4      1                  kirstyhawkshaw love the new song "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>suggesting potential conference keynote speake...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>jeffarchuleta glad the new members are meshing...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>nicramirez my wisdom tooth is growing in</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>i miss mcfly</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>kirstyhawkshaw love the new song</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":31}],"source":["df = df[['label', 'text']]\n","df.head()\n","\n",""]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["train_text, temp_text, train_labels, temp_labels = \\\n","    train_test_split(df['text'], df['label'], random_state=2018,\n","                     test_size=0.3, stratify=df['label'])\n","\n","\n","val_text, test_text, val_labels, test_labels = \\\n","    train_test_split(temp_text, temp_labels, random_state=2018,\n","                     test_size=0.5, stratify=temp_labels)\n","\n",""]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["bert = AutoModel.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",""]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["sent_id = tokenizer.batch_encode_plus(\n","    text, padding=True, return_token_type_ids=False)\n",""]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}],"source":["print(sent_id)\n",""]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AxesSubplot:>"]},"metadata":{},"execution_count":38},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.800027pt\" version=\"1.1\" viewBox=\"0 0 388.0125 248.800027\" width=\"388.0125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-10-27T22:04:18.930219</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.800027 \nL 388.0125 248.800027 \nL 388.0125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 46.0125 224.921902 \nL 380.8125 224.921902 \nL 380.8125 7.481902 \nL 46.0125 7.481902 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 61.230682 224.921902 \nL 71.376136 224.921902 \nL 71.376136 185.525901 \nL 61.230682 185.525901 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 71.376136 224.921902 \nL 81.521591 224.921902 \nL 81.521591 161.609345 \nL 71.376136 161.609345 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 81.521591 224.921902 \nL 91.667045 224.921902 \nL 91.667045 141.004312 \nL 81.521591 141.004312 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 91.667045 224.921902 \nL 101.8125 224.921902 \nL 101.8125 126.158078 \nL 91.667045 126.158078 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 101.8125 224.921902 \nL 111.957955 224.921902 \nL 111.957955 117.815093 \nL 101.8125 117.815093 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 111.957955 224.921902 \nL 122.103409 224.921902 \nL 122.103409 116.480215 \nL 111.957955 116.480215 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 122.103409 224.921902 \nL 132.248864 224.921902 \nL 132.248864 116.925175 \nL 122.103409 116.925175 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 132.248864 224.921902 \nL 142.394318 224.921902 \nL 142.394318 17.836188 \nL 132.248864 17.836188 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 142.394318 224.921902 \nL 152.539773 224.921902 \nL 152.539773 128.83639 \nL 142.394318 128.83639 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 152.539773 224.921902 \nL 162.685227 224.921902 \nL 162.685227 132.806795 \nL 152.539773 132.806795 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 162.685227 224.921902 \nL 172.830682 224.921902 \nL 172.830682 138.616935 \nL 162.685227 138.616935 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 172.830682 224.921902 \nL 182.976136 224.921902 \nL 182.976136 145.111627 \nL 172.830682 145.111627 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 182.976136 224.921902 \nL 193.121591 224.921902 \nL 193.121591 147.610244 \nL 182.976136 147.610244 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 193.121591 224.921902 \nL 203.267045 224.921902 \nL 203.267045 152.650263 \nL 193.121591 152.650263 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 203.267045 224.921902 \nL 213.4125 224.921902 \nL 213.4125 156.663452 \nL 203.267045 156.663452 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 213.4125 224.921902 \nL 223.557955 224.921902 \nL 223.557955 95.532906 \nL 213.4125 95.532906 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 223.557955 224.921902 \nL 233.703409 224.921902 \nL 233.703409 163.312169 \nL 223.557955 163.312169 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 233.703409 224.921902 \nL 243.848864 224.921902 \nL 243.848864 166.811944 \nL 233.703409 166.811944 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 243.848864 224.921902 \nL 253.994318 224.921902 \nL 253.994318 166.991639 \nL 243.848864 166.991639 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 253.994318 224.921902 \nL 264.139773 224.921902 \nL 264.139773 168.506212 \nL 253.994318 168.506212 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 264.139773 224.921902 \nL 274.285227 224.921902 \nL 274.285227 175.1806 \nL 264.139773 175.1806 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 274.285227 224.921902 \nL 284.430682 224.921902 \nL 284.430682 181.623951 \nL 274.285227 181.623951 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 284.430682 224.921902 \nL 294.576136 224.921902 \nL 294.576136 166.743489 \nL 284.430682 166.743489 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 294.576136 224.921902 \nL 304.721591 224.921902 \nL 304.721591 210.229692 \nL 294.576136 210.229692 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 304.721591 224.921902 \nL 314.867045 224.921902 \nL 314.867045 217.51162 \nL 304.721591 217.51162 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 314.867045 224.921902 \nL 325.0125 224.921902 \nL 325.0125 221.507696 \nL 314.867045 221.507696 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 325.0125 224.921902 \nL 335.157955 224.921902 \nL 335.157955 223.390216 \nL 325.0125 223.390216 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 335.157955 224.921902 \nL 345.303409 224.921902 \nL 345.303409 224.459829 \nL 335.157955 224.459829 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 345.303409 224.921902 \nL 355.448864 224.921902 \nL 355.448864 224.776435 \nL 345.303409 224.776435 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path clip-path=\"url(#p45cf766309)\" d=\"M 355.448864 224.921902 \nL 365.594318 224.921902 \nL 365.594318 224.862004 \nL 355.448864 224.862004 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p45cf766309)\" d=\"M 52.27881 224.921902 \nL 52.27881 7.481902 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m6a5248a5c5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.27881\" xlink:href=\"#m6a5248a5c5\" y=\"224.921902\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(49.09756 239.520339)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p45cf766309)\" d=\"M 97.038168 224.921902 \nL 97.038168 7.481902 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"97.038168\" xlink:href=\"#m6a5248a5c5\" y=\"224.921902\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(93.856918 239.520339)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p45cf766309)\" d=\"M 141.797527 224.921902 \nL 141.797527 7.481902 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"141.797527\" xlink:href=\"#m6a5248a5c5\" y=\"224.921902\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(135.435027 239.520339)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p45cf766309)\" d=\"M 186.556885 224.921902 \nL 186.556885 7.481902 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.556885\" xlink:href=\"#m6a5248a5c5\" y=\"224.921902\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(180.194385 239.520339)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p45cf766309)\" d=\"M 231.316243 224.921902 \nL 231.316243 7.481902 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"231.316243\" xlink:href=\"#m6a5248a5c5\" y=\"224.921902\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(224.953743 239.520339)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p45cf766309)\" d=\"M 276.075602 224.921902 \nL 276.075602 7.481902 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"276.075602\" xlink:href=\"#m6a5248a5c5\" y=\"224.921902\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(269.713102 239.520339)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p45cf766309)\" d=\"M 320.83496 224.921902 \nL 320.83496 7.481902 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"320.83496\" xlink:href=\"#m6a5248a5c5\" y=\"224.921902\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <g transform=\"translate(314.47246 239.520339)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p45cf766309)\" d=\"M 365.594318 224.921902 \nL 365.594318 7.481902 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"365.594318\" xlink:href=\"#m6a5248a5c5\" y=\"224.921902\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 35 -->\n      <g transform=\"translate(359.231818 239.520339)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p45cf766309)\" d=\"M 46.0125 224.921902 \nL 380.8125 224.921902 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"md9be1a1f68\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#md9be1a1f68\" y=\"224.921902\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0 -->\n      <g transform=\"translate(32.65 228.721121)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p45cf766309)\" d=\"M 46.0125 182.137365 \nL 380.8125 182.137365 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#md9be1a1f68\" y=\"182.137365\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 5000 -->\n      <g transform=\"translate(13.5625 185.936584)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p45cf766309)\" d=\"M 46.0125 139.352829 \nL 380.8125 139.352829 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#md9be1a1f68\" y=\"139.352829\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 10000 -->\n      <g transform=\"translate(7.2 143.152047)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p45cf766309)\" d=\"M 46.0125 96.568292 \nL 380.8125 96.568292 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#md9be1a1f68\" y=\"96.568292\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 15000 -->\n      <g transform=\"translate(7.2 100.367511)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p45cf766309)\" d=\"M 46.0125 53.783755 \nL 380.8125 53.783755 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#md9be1a1f68\" y=\"53.783755\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 20000 -->\n      <g transform=\"translate(7.2 57.582974)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p45cf766309)\" d=\"M 46.0125 10.999219 \nL 380.8125 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#md9be1a1f68\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 25000 -->\n      <g transform=\"translate(7.2 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 46.0125 224.921902 \nL 46.0125 7.481902 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 380.8125 224.921902 \nL 380.8125 7.481902 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 46.0125 224.921902 \nL 380.8125 224.921902 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 46.0125 7.481902 \nL 380.8125 7.481902 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p45cf766309\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"46.0125\" y=\"7.481902\"/>\n  </clipPath>\n </defs>\n</svg>\n","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["seq_len = [len(i.split()) for i in train_text]\n","\n","pd.Series(seq_len).hist(bins=30)\n",""]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["max_seq_len = 30\n",""]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["\n","# for train set\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","# for validation set\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","# for test set\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())\n",""]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["\n","# define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(\n","    train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(\n","    val_data, sampler=val_sampler, batch_size=batch_size)\n",""]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["\n","# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = False\n",""]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["\n","\n","class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","        super(BERT_Arch, self).__init__()\n","        self.bert = bert\n","        # dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","        # relu activation function\n","        self.relu = nn.ReLU()\n","        # dense layer 1\n","        self.fc1 = nn.Linear(768, 512)\n","        # dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512, 2)\n","        # softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    # define the forward pass\n","    def forward(self, sent_id, mask):\n","        # pass the inputs to the model\n","        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n","        x = self.fc1(cls_hs)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        # output layer\n","        x = self.fc2(x)\n","        # apply softmax activation\n","        x = self.softmax(x)\n","        return x\n","\n",""]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["model = BERT_Arch(bert)\n",""]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["model = model.to(device)\n","\n","# optimizer from hugging face transformers\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr=1e-3)\n",""]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.99531053 1.00473387]\n"]}],"source":["\n","\n","# compute the class weights\n","class_wts = compute_class_weight(\n","    'balanced', np.unique(train_labels), train_labels)\n","\n","print(class_wts)\n","\n","# convert class weights to tensor\n","weights = torch.tensor(class_wts, dtype=torch.float)\n","weights = weights.to(device)\n","\n","# loss function\n","cross_entropy = nn.NLLLoss(weight=weights)\n","\n","# number of training epochs\n","epochs = 10\n",""]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["\n","# function to train the model\n","\n","\n","def train():\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","    # empty list to save model predictions\n","    total_preds = []\n","    # iterate over batches\n","    for step, batch in enumerate(train_dataloader):\n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(\n","                step, len(train_dataloader)))\n","        # push the batch to gpu\n","        batch = [r.to(device) for r in batch]\n","        sent_id, mask, labels = batch\n","        # clear previously calculated gradients\n","        model.zero_grad()\n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask)\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","        # add on to the total loss\n","        total_loss = total_loss + loss.item()\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        # update parameters\n","        optimizer.step()\n","        # model predictions are stored on GPU. So, push it to CPU\n","        preds = preds.detach().cpu().numpy()\n","        # append the model predictions\n","        total_preds.append(preds)\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","    # returns the loss and predictions\n","    return avg_loss, total_preds\n","\n","\n","# function for evaluating the model\n","def evaluate():\n","\n","    print(\"\\nEvaluating...\")\n","\n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save the model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    for step, batch in enumerate(val_dataloader):\n","\n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","\n","            # Calculate elapsed time in minutes.\n","            # elapsed = format_time(time.time() - t0)\n","\n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(\n","                step, len(val_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","\n","            # model predictions\n","            preds = model(sent_id, mask)\n","\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds, labels)\n","\n","            total_loss = total_loss + loss.item()\n","\n","            preds = preds.detach().cpu().numpy()\n","\n","            total_preds.append(preds)\n","\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(val_dataloader)\n","\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","\n","    return avg_loss, total_preds\n","\n",""]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Epoch 1 / 10\n","  Batch    50  of  7,051.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m~/Documents/python-projects/nlp-transfer-learnin-test/notebook2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/python-projects/nlp-transfer-learnin-test/notebook2.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# add on to the total loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# backward pass to calculate the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses = []\n","valid_losses = []\n","\n","# for each epoch\n","for epoch in range(epochs):\n","\n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","\n","    # train model\n","    train_loss, _ = train()\n","\n","    # evaluate model\n","    valid_loss, _ = evaluate()\n","\n","    # save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","\n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')\n",""]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","from transformers import AdamW\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from IPython.core.interactiveshell import InteractiveShell\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","import time\n","\n","# specify GPU\n","device = torch.device(\"cuda\")\n","\n","# This is for multiple print statements per cell\n","InteractiveShell.ast_node_interactivity = \"all\"\n",""]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text\n","0  negative                       oh no its fading away again \n","1  positive  bunnylake will kill me but i cant stop listeni...\n","2  negative  last day in cali  partyin for the last time wi...\n","3  negative                     is having a major soar throat \n","4  positive                       my last day as 12 years old "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>oh no its fading away again</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>bunnylake will kill me but i cant stop listeni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>last day in cali  partyin for the last time wi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>negative</td>\n      <td>is having a major soar throat</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>my last day as 12 years old</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":53}],"source":["df = pd.read_csv(\"pre-cleaned_consolidated_tweet_data.csv\", sep='\\t')\n","df.head()\n",""]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["frac = 0.2\n","# sample and shuffle the dataset according to the fraction choise in the line above\n","df = df.sample(frac=frac).reset_index(drop=True)\n","\n",""]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text  label\n","0  negative                          only one baby bunny left       0\n","1  negative  typicaldoll miss u so much girl  and love u so...      0\n","2  negative  adamcolas adammmmmmm whats uppppp we dont talk...      0\n","3  positive  gnarrly yeah infinite exam period with no exam...      1\n","4  positive  juniperus aww those kids  im good sipping my i...      1"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>only one baby bunny left</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>negative</td>\n      <td>typicaldoll miss u so much girl  and love u so...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>adamcolas adammmmmmm whats uppppp we dont talk...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>positive</td>\n      <td>gnarrly yeah infinite exam period with no exam...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>juniperus aww those kids  im good sipping my i...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":55}],"source":["df['label'] = df.apply(lambda x: 1 if x['sentiment']\n","                       == 'positive' else 0, axis=1)\n","df.head()\n",""]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   label                                               text\n","0      0                          only one baby bunny left \n","1      0  typicaldoll miss u so much girl  and love u so...\n","2      0  adamcolas adammmmmmm whats uppppp we dont talk...\n","3      1  gnarrly yeah infinite exam period with no exam...\n","4      1  juniperus aww those kids  im good sipping my i..."],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>only one baby bunny left</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>typicaldoll miss u so much girl  and love u so...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>adamcolas adammmmmmm whats uppppp we dont talk...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>gnarrly yeah infinite exam period with no exam...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>juniperus aww those kids  im good sipping my i...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":56}],"source":["df = df[['label', 'text']]\n","df.head()\n","\n",""]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["train_text, temp_text, train_labels, temp_labels = \\\n","    train_test_split(df['text'], df['label'], random_state=2018,\n","                     test_size=0.3, stratify=df['label'])\n","\n","\n","val_text, test_text, val_labels, test_labels = \\\n","    train_test_split(temp_text, temp_labels, random_state=2018,\n","                     test_size=0.5, stratify=temp_labels)\n","\n",""]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["bert = AutoModel.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",""]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["sent_id = tokenizer.batch_encode_plus(\n","    text, padding=True, return_token_type_ids=False)\n",""]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}],"source":["print(sent_id)\n",""]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AxesSubplot:>"]},"metadata":{},"execution_count":63},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 388.0125 248.518125\" width=\"388.0125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-10-27T22:06:46.037493</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 388.0125 248.518125 \nL 388.0125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 46.0125 224.64 \nL 380.8125 224.64 \nL 380.8125 7.2 \nL 46.0125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 61.230682 224.64 \nL 71.376136 224.64 \nL 71.376136 185.547675 \nL 61.230682 185.547675 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 71.376136 224.64 \nL 81.521591 224.64 \nL 81.521591 161.364511 \nL 71.376136 161.364511 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 81.521591 224.64 \nL 91.667045 224.64 \nL 91.667045 142.016296 \nL 81.521591 142.016296 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 91.667045 224.64 \nL 101.8125 224.64 \nL 101.8125 127.511451 \nL 91.667045 127.511451 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 101.8125 224.64 \nL 111.957955 224.64 \nL 111.957955 119.332471 \nL 101.8125 119.332471 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 111.957955 224.64 \nL 122.103409 224.64 \nL 122.103409 116.61176 \nL 111.957955 116.61176 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 122.103409 224.64 \nL 132.248864 224.64 \nL 132.248864 17.554286 \nL 122.103409 17.554286 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 132.248864 224.64 \nL 142.394318 224.64 \nL 142.394318 125.439331 \nL 132.248864 125.439331 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 142.394318 224.64 \nL 152.539773 224.64 \nL 152.539773 128.867595 \nL 142.394318 128.867595 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 152.539773 224.64 \nL 162.685227 224.64 \nL 162.685227 134.309018 \nL 152.539773 134.309018 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 162.685227 224.64 \nL 172.830682 224.64 \nL 172.830682 139.573552 \nL 162.685227 139.573552 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 172.830682 224.64 \nL 182.976136 224.64 \nL 182.976136 146.152114 \nL 172.830682 146.152114 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 182.976136 224.64 \nL 193.121591 224.64 \nL 193.121591 78.521802 \nL 182.976136 78.521802 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 193.121591 224.64 \nL 203.267045 224.64 \nL 203.267045 157.708819 \nL 193.121591 157.708819 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 203.267045 224.64 \nL 213.4125 224.64 \nL 213.4125 160.168409 \nL 203.267045 160.168409 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 213.4125 224.64 \nL 223.557955 224.64 \nL 223.557955 161.381358 \nL 213.4125 161.381358 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 223.557955 224.64 \nL 233.703409 224.64 \nL 233.703409 163.731446 \nL 223.557955 163.731446 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 233.703409 224.64 \nL 243.848864 224.64 \nL 243.848864 166.115227 \nL 233.703409 166.115227 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 243.848864 224.64 \nL 253.994318 224.64 \nL 253.994318 113.090839 \nL 243.848864 113.090839 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 253.994318 224.64 \nL 264.139773 224.64 \nL 264.139773 175.574542 \nL 253.994318 175.574542 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 264.139773 224.64 \nL 274.285227 224.64 \nL 274.285227 181.462397 \nL 264.139773 181.462397 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 274.285227 224.64 \nL 284.430682 224.64 \nL 284.430682 190.972251 \nL 274.285227 190.972251 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 284.430682 224.64 \nL 294.576136 224.64 \nL 294.576136 201.400241 \nL 284.430682 201.400241 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 294.576136 224.64 \nL 304.721591 224.64 \nL 304.721591 210.152002 \nL 294.576136 210.152002 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 304.721591 224.64 \nL 314.867045 224.64 \nL 314.867045 213.437071 \nL 304.721591 213.437071 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 314.867045 224.64 \nL 325.0125 224.64 \nL 325.0125 223.005889 \nL 314.867045 223.005889 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 325.0125 224.64 \nL 335.157955 224.64 \nL 335.157955 224.067219 \nL 325.0125 224.067219 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 335.157955 224.64 \nL 345.303409 224.64 \nL 345.303409 224.530498 \nL 335.157955 224.530498 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 345.303409 224.64 \nL 355.448864 224.64 \nL 355.448864 224.572614 \nL 345.303409 224.572614 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path clip-path=\"url(#pced7ad2685)\" d=\"M 355.448864 224.64 \nL 365.594318 224.64 \nL 365.594318 224.61473 \nL 355.448864 224.61473 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pced7ad2685)\" d=\"M 52.534578 224.64 \nL 52.534578 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mcd43fe8e4a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.534578\" xlink:href=\"#mcd43fe8e4a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(49.353328 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pced7ad2685)\" d=\"M 96.015097 224.64 \nL 96.015097 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.015097\" xlink:href=\"#mcd43fe8e4a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(92.833847 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pced7ad2685)\" d=\"M 139.495617 224.64 \nL 139.495617 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"139.495617\" xlink:href=\"#mcd43fe8e4a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(133.133117 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pced7ad2685)\" d=\"M 182.976136 224.64 \nL 182.976136 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.976136\" xlink:href=\"#mcd43fe8e4a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(176.613636 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pced7ad2685)\" d=\"M 226.456656 224.64 \nL 226.456656 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"226.456656\" xlink:href=\"#mcd43fe8e4a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(220.094156 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pced7ad2685)\" d=\"M 269.937175 224.64 \nL 269.937175 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"269.937175\" xlink:href=\"#mcd43fe8e4a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(263.574675 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pced7ad2685)\" d=\"M 313.417695 224.64 \nL 313.417695 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"313.417695\" xlink:href=\"#mcd43fe8e4a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <g transform=\"translate(307.055195 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#pced7ad2685)\" d=\"M 356.898214 224.64 \nL 356.898214 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"356.898214\" xlink:href=\"#mcd43fe8e4a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 35 -->\n      <g transform=\"translate(350.535714 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#pced7ad2685)\" d=\"M 46.0125 224.64 \nL 380.8125 224.64 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m8c222f9da1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m8c222f9da1\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0 -->\n      <g transform=\"translate(32.65 228.439219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#pced7ad2685)\" d=\"M 46.0125 182.523727 \nL 380.8125 182.523727 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m8c222f9da1\" y=\"182.523727\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 5000 -->\n      <g transform=\"translate(13.5625 186.322946)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#pced7ad2685)\" d=\"M 46.0125 140.407454 \nL 380.8125 140.407454 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m8c222f9da1\" y=\"140.407454\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 10000 -->\n      <g transform=\"translate(7.2 144.206673)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#pced7ad2685)\" d=\"M 46.0125 98.291181 \nL 380.8125 98.291181 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m8c222f9da1\" y=\"98.291181\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 15000 -->\n      <g transform=\"translate(7.2 102.0904)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#pced7ad2685)\" d=\"M 46.0125 56.174908 \nL 380.8125 56.174908 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m8c222f9da1\" y=\"56.174908\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 20000 -->\n      <g transform=\"translate(7.2 59.974127)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#pced7ad2685)\" d=\"M 46.0125 14.058635 \nL 380.8125 14.058635 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m8c222f9da1\" y=\"14.058635\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 25000 -->\n      <g transform=\"translate(7.2 17.857854)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 46.0125 224.64 \nL 46.0125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 380.8125 224.64 \nL 380.8125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 46.0125 224.64 \nL 380.8125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 46.0125 7.2 \nL 380.8125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pced7ad2685\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"46.0125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["seq_len = [len(i.split()) for i in train_text]\n","\n","pd.Series(seq_len).hist(bins=30)\n",""]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["max_seq_len = 30\n",""]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["\n","# for train set\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","# for validation set\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","# for test set\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())\n",""]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["\n","# define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(\n","    train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(\n","    val_data, sampler=val_sampler, batch_size=batch_size)\n",""]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["\n","# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = False\n",""]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["\n","\n","class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","        super(BERT_Arch, self).__init__()\n","        self.bert = bert\n","        # dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","        # relu activation function\n","        self.relu = nn.ReLU()\n","        # dense layer 1\n","        self.fc1 = nn.Linear(768, 512)\n","        # dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512, 2)\n","        # softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    # define the forward pass\n","    def forward(self, sent_id, mask):\n","        # pass the inputs to the model\n","        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n","        x = self.fc1(cls_hs)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        # output layer\n","        x = self.fc2(x)\n","        # apply softmax activation\n","        x = self.softmax(x)\n","        return x\n","\n",""]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["model = BERT_Arch(bert)\n",""]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["model = model.to(device)\n","\n","# optimizer from hugging face transformers\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr=1e-3)\n",""]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.99313742 1.00695808]\n"]}],"source":["\n","\n","# compute the class weights\n","class_wts = compute_class_weight(\n","    'balanced', np.unique(train_labels), train_labels)\n","\n","print(class_wts)\n","\n","# convert class weights to tensor\n","weights = torch.tensor(class_wts, dtype=torch.float)\n","weights = weights.to(device)\n","\n","# loss function\n","cross_entropy = nn.NLLLoss(weight=weights)\n","\n","# number of training epochs\n","epochs = 10\n",""]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["\n","# function to train the model\n","\n","\n","def train():\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","    # empty list to save model predictions\n","    total_preds = []\n","    # iterate over batches\n","    for step, batch in enumerate(train_dataloader):\n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(\n","                step, len(train_dataloader)))\n","        # push the batch to gpu\n","        batch = [r.to(device) for r in batch]\n","        sent_id, mask, labels = batch\n","        # clear previously calculated gradients\n","        model.zero_grad()\n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask)\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","        # add on to the total loss\n","        total_loss = total_loss + loss.item()\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        # update parameters\n","        optimizer.step()\n","        # model predictions are stored on GPU. So, push it to CPU\n","        preds = preds.detach().cpu().numpy()\n","        # append the model predictions\n","        total_preds.append(preds)\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","    # returns the loss and predictions\n","    return avg_loss, total_preds\n","\n","\n","# function for evaluating the model\n","def evaluate():\n","\n","    print(\"\\nEvaluating...\")\n","\n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save the model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    for step, batch in enumerate(val_dataloader):\n","\n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","\n","            # Calculate elapsed time in minutes.\n","            # elapsed = format_time(time.time() - t0)\n","\n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(\n","                step, len(val_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","\n","            # model predictions\n","            preds = model(sent_id, mask)\n","\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds, labels)\n","\n","            total_loss = total_loss + loss.item()\n","\n","            preds = preds.detach().cpu().numpy()\n","\n","            total_preds.append(preds)\n","\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(val_dataloader)\n","\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","\n","    return avg_loss, total_preds\n","\n",""]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Epoch 1 / 10\n","  Batch    50  of  7,051.\n","  Batch   100  of  7,051.\n","  Batch   150  of  7,051.\n","  Batch   200  of  7,051.\n","  Batch   250  of  7,051.\n","  Batch   300  of  7,051.\n","  Batch   350  of  7,051.\n","  Batch   400  of  7,051.\n","  Batch   450  of  7,051.\n","  Batch   500  of  7,051.\n","  Batch   550  of  7,051.\n","  Batch   600  of  7,051.\n","  Batch   650  of  7,051.\n","  Batch   700  of  7,051.\n","  Batch   750  of  7,051.\n","  Batch   800  of  7,051.\n","  Batch   850  of  7,051.\n","  Batch   900  of  7,051.\n","  Batch   950  of  7,051.\n","  Batch 1,000  of  7,051.\n","  Batch 1,050  of  7,051.\n","  Batch 1,100  of  7,051.\n","  Batch 1,150  of  7,051.\n","  Batch 1,200  of  7,051.\n","  Batch 1,250  of  7,051.\n","  Batch 1,300  of  7,051.\n","  Batch 1,350  of  7,051.\n","  Batch 1,400  of  7,051.\n","  Batch 1,450  of  7,051.\n","  Batch 1,500  of  7,051.\n","  Batch 1,550  of  7,051.\n","  Batch 1,600  of  7,051.\n","  Batch 1,650  of  7,051.\n","  Batch 1,700  of  7,051.\n","  Batch 1,750  of  7,051.\n","  Batch 1,800  of  7,051.\n","  Batch 1,850  of  7,051.\n","  Batch 1,900  of  7,051.\n","  Batch 1,950  of  7,051.\n","  Batch 2,000  of  7,051.\n","  Batch 2,050  of  7,051.\n","  Batch 2,100  of  7,051.\n","  Batch 2,150  of  7,051.\n","  Batch 2,200  of  7,051.\n","  Batch 2,250  of  7,051.\n","  Batch 2,300  of  7,051.\n","  Batch 2,350  of  7,051.\n","  Batch 2,400  of  7,051.\n","  Batch 2,450  of  7,051.\n","  Batch 2,500  of  7,051.\n","  Batch 2,550  of  7,051.\n","  Batch 2,600  of  7,051.\n","  Batch 2,650  of  7,051.\n","  Batch 2,700  of  7,051.\n","  Batch 2,750  of  7,051.\n","  Batch 2,800  of  7,051.\n","  Batch 2,850  of  7,051.\n","  Batch 2,900  of  7,051.\n","  Batch 2,950  of  7,051.\n","  Batch 3,000  of  7,051.\n","  Batch 3,050  of  7,051.\n","  Batch 3,100  of  7,051.\n","  Batch 3,150  of  7,051.\n","  Batch 3,200  of  7,051.\n","  Batch 3,250  of  7,051.\n","  Batch 3,300  of  7,051.\n","  Batch 3,350  of  7,051.\n","  Batch 3,400  of  7,051.\n","  Batch 3,450  of  7,051.\n","  Batch 3,500  of  7,051.\n","  Batch 3,550  of  7,051.\n","  Batch 3,600  of  7,051.\n","  Batch 3,650  of  7,051.\n","  Batch 3,700  of  7,051.\n","  Batch 3,750  of  7,051.\n","  Batch 3,800  of  7,051.\n","  Batch 3,850  of  7,051.\n","  Batch 3,900  of  7,051.\n","  Batch 3,950  of  7,051.\n","  Batch 4,000  of  7,051.\n","  Batch 4,050  of  7,051.\n","  Batch 4,100  of  7,051.\n","  Batch 4,150  of  7,051.\n","  Batch 4,200  of  7,051.\n","  Batch 4,250  of  7,051.\n","  Batch 4,300  of  7,051.\n","  Batch 4,350  of  7,051.\n","  Batch 4,400  of  7,051.\n","  Batch 4,450  of  7,051.\n","  Batch 4,500  of  7,051.\n","  Batch 4,550  of  7,051.\n","  Batch 4,600  of  7,051.\n","  Batch 4,650  of  7,051.\n","  Batch 4,700  of  7,051.\n","  Batch 4,750  of  7,051.\n","  Batch 4,800  of  7,051.\n","  Batch 4,850  of  7,051.\n","  Batch 4,900  of  7,051.\n","  Batch 4,950  of  7,051.\n","  Batch 5,000  of  7,051.\n","  Batch 5,050  of  7,051.\n","  Batch 5,100  of  7,051.\n","  Batch 5,150  of  7,051.\n","  Batch 5,200  of  7,051.\n","  Batch 5,250  of  7,051.\n","  Batch 5,300  of  7,051.\n","  Batch 5,350  of  7,051.\n","  Batch 5,400  of  7,051.\n","  Batch 5,450  of  7,051.\n","  Batch 5,500  of  7,051.\n","  Batch 5,550  of  7,051.\n","  Batch 5,600  of  7,051.\n","  Batch 5,650  of  7,051.\n","  Batch 5,700  of  7,051.\n","  Batch 5,750  of  7,051.\n","  Batch 5,800  of  7,051.\n","  Batch 5,850  of  7,051.\n","  Batch 5,900  of  7,051.\n","  Batch 5,950  of  7,051.\n","  Batch 6,000  of  7,051.\n","  Batch 6,050  of  7,051.\n","  Batch 6,100  of  7,051.\n","  Batch 6,150  of  7,051.\n","  Batch 6,200  of  7,051.\n","  Batch 6,250  of  7,051.\n","  Batch 6,300  of  7,051.\n","  Batch 6,350  of  7,051.\n","  Batch 6,400  of  7,051.\n","  Batch 6,450  of  7,051.\n","  Batch 6,500  of  7,051.\n","  Batch 6,550  of  7,051.\n","  Batch 6,600  of  7,051.\n","  Batch 6,650  of  7,051.\n","  Batch 6,700  of  7,051.\n","  Batch 6,750  of  7,051.\n","  Batch 6,800  of  7,051.\n","  Batch 6,850  of  7,051.\n","  Batch 6,900  of  7,051.\n","  Batch 6,950  of  7,051.\n","  Batch 7,000  of  7,051.\n","  Batch 7,050  of  7,051.\n","\n","Evaluating...\n","  Batch    50  of  1,511.\n","  Batch   100  of  1,511.\n","  Batch   150  of  1,511.\n","  Batch   200  of  1,511.\n","  Batch   250  of  1,511.\n","  Batch   300  of  1,511.\n","  Batch   350  of  1,511.\n","  Batch   400  of  1,511.\n","  Batch   450  of  1,511.\n","  Batch   500  of  1,511.\n","  Batch   550  of  1,511.\n","  Batch   600  of  1,511.\n","  Batch   650  of  1,511.\n","  Batch   700  of  1,511.\n","  Batch   750  of  1,511.\n","  Batch   800  of  1,511.\n","  Batch   850  of  1,511.\n","  Batch   900  of  1,511.\n","  Batch   950  of  1,511.\n","  Batch 1,000  of  1,511.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m~/Documents/python-projects/nlp-transfer-learnin-test/notebook2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# save the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/python-projects/nlp-transfer-learnin-test/notebook2.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses = []\n","valid_losses = []\n","\n","# for each epoch\n","for epoch in range(epochs):\n","\n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","\n","    # train model\n","    train_loss, _ = train()\n","\n","    # evaluate model\n","    valid_loss, _ = evaluate()\n","\n","    # save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","\n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')\n",""]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","from transformers import AdamW\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from IPython.core.interactiveshell import InteractiveShell\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","import time\n","\n","# specify GPU\n","device = torch.device(\"cuda\")\n","\n","# This is for multiple print statements per cell\n","InteractiveShell.ast_node_interactivity = \"all\"\n",""]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text\n","0  negative                       oh no its fading away again \n","1  positive  bunnylake will kill me but i cant stop listeni...\n","2  negative  last day in cali  partyin for the last time wi...\n","3  negative                     is having a major soar throat \n","4  positive                       my last day as 12 years old "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>oh no its fading away again</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>bunnylake will kill me but i cant stop listeni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>last day in cali  partyin for the last time wi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>negative</td>\n      <td>is having a major soar throat</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>my last day as 12 years old</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":78}],"source":["df = pd.read_csv(\"pre-cleaned_consolidated_tweet_data.csv\", sep='\\t')\n","df.head()\n",""]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["frac = 0.02\n","# sample and shuffle the dataset according to the fraction choise in the line above\n","df = df.sample(frac=frac).reset_index(drop=True)\n","\n",""]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text  label\n","0  negative  bernadinee can u take the photo of box collect...      0\n","1  positive                               killing electronica       1\n","2  positive  off to sorrento in 4 weeks  tanning eating dri...      1\n","3  negative  nikkibenz you mean you couldnt get out of the ...      0\n","4  positive  donniescupcake you still have a week girl take...      1"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>bernadinee can u take the photo of box collect...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>killing electronica</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>positive</td>\n      <td>off to sorrento in 4 weeks  tanning eating dri...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>negative</td>\n      <td>nikkibenz you mean you couldnt get out of the ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>donniescupcake you still have a week girl take...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":80}],"source":["df['label'] = df.apply(lambda x: 1 if x['sentiment']\n","                       == 'positive' else 0, axis=1)\n","df.head()\n",""]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   label                                               text\n","0      0  bernadinee can u take the photo of box collect...\n","1      1                               killing electronica \n","2      1  off to sorrento in 4 weeks  tanning eating dri...\n","3      0  nikkibenz you mean you couldnt get out of the ...\n","4      1  donniescupcake you still have a week girl take..."],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>bernadinee can u take the photo of box collect...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>killing electronica</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>off to sorrento in 4 weeks  tanning eating dri...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>nikkibenz you mean you couldnt get out of the ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>donniescupcake you still have a week girl take...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":81}],"source":["df = df[['label', 'text']]\n","df.head()\n","\n",""]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["train_text, temp_text, train_labels, temp_labels = \\\n","    train_test_split(df['text'], df['label'], random_state=2018,\n","                     test_size=0.3, stratify=df['label'])\n","\n","\n","val_text, test_text, val_labels, test_labels = \\\n","    train_test_split(temp_text, temp_labels, random_state=2018,\n","                     test_size=0.5, stratify=temp_labels)\n","\n",""]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["bert = AutoModel.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",""]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":["sent_id = tokenizer.batch_encode_plus(\n","    text, padding=True, return_token_type_ids=False)\n",""]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}],"source":["print(sent_id)\n",""]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AxesSubplot:>"]},"metadata":{},"execution_count":88},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 381.65 248.518125\" width=\"381.65pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-10-27T22:45:53.559965</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 248.518125 \nL 381.65 248.518125 \nL 381.65 0 \nL -0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 39.65 224.64 \nL 374.45 224.64 \nL 374.45 7.2 \nL 39.65 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 54.868182 224.64 \nL 65.013636 224.64 \nL 65.013636 159.481764 \nL 54.868182 159.481764 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 65.013636 224.64 \nL 75.159091 224.64 \nL 75.159091 129.566348 \nL 65.013636 129.566348 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 75.159091 224.64 \nL 85.304545 224.64 \nL 85.304545 87.630124 \nL 75.159091 87.630124 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 85.304545 224.64 \nL 95.45 224.64 \nL 95.45 74.789717 \nL 85.304545 74.789717 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 95.45 224.64 \nL 105.595455 224.64 \nL 105.595455 61.40291 \nL 95.45 61.40291 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 105.595455 224.64 \nL 115.740909 224.64 \nL 115.740909 52.114105 \nL 105.595455 52.114105 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 115.740909 224.64 \nL 125.886364 224.64 \nL 125.886364 49.928504 \nL 115.740909 49.928504 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 125.886364 224.64 \nL 136.031818 224.64 \nL 136.031818 51.431104 \nL 125.886364 51.431104 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 136.031818 224.64 \nL 146.177273 224.64 \nL 146.177273 64.408112 \nL 136.031818 64.408112 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 146.177273 224.64 \nL 156.322727 224.64 \nL 156.322727 67.276713 \nL 146.177273 67.276713 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 156.322727 224.64 \nL 166.468182 224.64 \nL 166.468182 78.477919 \nL 156.322727 78.477919 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 166.468182 224.64 \nL 176.613636 224.64 \nL 176.613636 90.908526 \nL 166.468182 90.908526 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 176.613636 224.64 \nL 186.759091 224.64 \nL 186.759091 96.509129 \nL 176.613636 96.509129 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 186.759091 224.64 \nL 196.904545 224.64 \nL 196.904545 105.934534 \nL 186.759091 105.934534 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 196.904545 224.64 \nL 207.05 224.64 \nL 207.05 109.486136 \nL 196.904545 109.486136 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 207.05 224.64 \nL 217.195455 224.64 \nL 217.195455 17.554286 \nL 207.05 17.554286 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 217.195455 224.64 \nL 227.340909 224.64 \nL 227.340909 123.692544 \nL 217.195455 123.692544 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 227.340909 224.64 \nL 237.486364 224.64 \nL 237.486364 126.424546 \nL 227.340909 126.424546 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 237.486364 224.64 \nL 247.631818 224.64 \nL 247.631818 122.463144 \nL 237.486364 122.463144 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 247.631818 224.64 \nL 257.777273 224.64 \nL 257.777273 128.063747 \nL 247.631818 128.063747 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 257.777273 224.64 \nL 267.922727 224.64 \nL 267.922727 138.035552 \nL 257.777273 138.035552 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 267.922727 224.64 \nL 278.068182 224.64 \nL 278.068182 138.445352 \nL 267.922727 138.445352 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 278.068182 224.64 \nL 288.213636 224.64 \nL 288.213636 154.564161 \nL 278.068182 154.564161 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 288.213636 224.64 \nL 298.359091 224.64 \nL 298.359091 165.082367 \nL 288.213636 165.082367 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 298.359091 224.64 \nL 308.504545 224.64 \nL 308.504545 183.796577 \nL 298.359091 183.796577 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 308.504545 224.64 \nL 318.65 224.64 \nL 318.65 202.920588 \nL 308.504545 202.920588 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 318.65 224.64 \nL 328.795455 224.64 \nL 328.795455 212.345993 \nL 318.65 212.345993 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 328.795455 224.64 \nL 338.940909 224.64 \nL 338.940909 218.629597 \nL 328.795455 218.629597 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 338.940909 224.64 \nL 349.086364 224.64 \nL 349.086364 222.727599 \nL 338.940909 222.727599 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path clip-path=\"url(#p70082239ce)\" d=\"M 349.086364 224.64 \nL 359.231818 224.64 \nL 359.231818 223.273999 \nL 349.086364 223.273999 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p70082239ce)\" d=\"M 45.356818 224.64 \nL 45.356818 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m646aa5d10b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.356818\" xlink:href=\"#m646aa5d10b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(42.175568 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p70082239ce)\" d=\"M 92.913636 224.64 \nL 92.913636 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"92.913636\" xlink:href=\"#m646aa5d10b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(89.732386 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p70082239ce)\" d=\"M 140.470455 224.64 \nL 140.470455 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"140.470455\" xlink:href=\"#m646aa5d10b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(134.107955 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p70082239ce)\" d=\"M 188.027273 224.64 \nL 188.027273 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"188.027273\" xlink:href=\"#m646aa5d10b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(181.664773 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p70082239ce)\" d=\"M 235.584091 224.64 \nL 235.584091 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.584091\" xlink:href=\"#m646aa5d10b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(229.221591 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p70082239ce)\" d=\"M 283.140909 224.64 \nL 283.140909 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"283.140909\" xlink:href=\"#m646aa5d10b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(276.778409 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p70082239ce)\" d=\"M 330.697727 224.64 \nL 330.697727 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.697727\" xlink:href=\"#m646aa5d10b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <g transform=\"translate(324.335227 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p70082239ce)\" d=\"M 39.65 224.64 \nL 374.45 224.64 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mca6ff53105\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mca6ff53105\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g transform=\"translate(26.2875 228.439219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p70082239ce)\" d=\"M 39.65 197.319985 \nL 374.45 197.319985 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mca6ff53105\" y=\"197.319985\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 200 -->\n      <g transform=\"translate(13.5625 201.119204)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p70082239ce)\" d=\"M 39.65 169.99997 \nL 374.45 169.99997 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mca6ff53105\" y=\"169.99997\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 400 -->\n      <g transform=\"translate(13.5625 173.799189)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p70082239ce)\" d=\"M 39.65 142.679955 \nL 374.45 142.679955 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mca6ff53105\" y=\"142.679955\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 600 -->\n      <g transform=\"translate(13.5625 146.479174)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p70082239ce)\" d=\"M 39.65 115.35994 \nL 374.45 115.35994 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mca6ff53105\" y=\"115.35994\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 800 -->\n      <g transform=\"translate(13.5625 119.159158)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p70082239ce)\" d=\"M 39.65 88.039925 \nL 374.45 88.039925 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mca6ff53105\" y=\"88.039925\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1000 -->\n      <g transform=\"translate(7.2 91.839143)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p70082239ce)\" d=\"M 39.65 60.71991 \nL 374.45 60.71991 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mca6ff53105\" y=\"60.71991\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1200 -->\n      <g transform=\"translate(7.2 64.519128)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p70082239ce)\" d=\"M 39.65 33.399894 \nL 374.45 33.399894 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mca6ff53105\" y=\"33.399894\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 1400 -->\n      <g transform=\"translate(7.2 37.199113)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 39.65 224.64 \nL 39.65 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 374.45 224.64 \nL 374.45 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 39.65 224.64 \nL 374.45 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 39.65 7.2 \nL 374.45 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p70082239ce\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"39.65\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["seq_len = [len(i.split()) for i in train_text]\n","\n","pd.Series(seq_len).hist(bins=30)\n",""]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["max_seq_len = 30\n",""]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[],"source":["tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":["\n","# for train set\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","# for validation set\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","# for test set\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())\n",""]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["\n","# define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(\n","    train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(\n","    val_data, sampler=val_sampler, batch_size=batch_size)\n",""]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["\n","# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = False\n",""]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[],"source":["\n","\n","class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","        super(BERT_Arch, self).__init__()\n","        self.bert = bert\n","        # dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","        # relu activation function\n","        self.relu = nn.ReLU()\n","        # dense layer 1\n","        self.fc1 = nn.Linear(768, 512)\n","        # dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512, 2)\n","        # softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    # define the forward pass\n","    def forward(self, sent_id, mask):\n","        # pass the inputs to the model\n","        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n","        x = self.fc1(cls_hs)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        # output layer\n","        x = self.fc2(x)\n","        # apply softmax activation\n","        x = self.softmax(x)\n","        return x\n","\n",""]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[],"source":["model = BERT_Arch(bert)\n",""]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[],"source":["model = model.to(device)\n","\n","# optimizer from hugging face transformers\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr=1e-3)\n",""]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.98536862 1.01507244]\n"]}],"source":["\n","\n","# compute the class weights\n","class_wts = compute_class_weight(\n","    'balanced', np.unique(train_labels), train_labels)\n","\n","print(class_wts)\n","\n","# convert class weights to tensor\n","weights = torch.tensor(class_wts, dtype=torch.float)\n","weights = weights.to(device)\n","\n","# loss function\n","cross_entropy = nn.NLLLoss(weight=weights)\n","\n","# number of training epochs\n","epochs = 10\n",""]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[],"source":["\n","# function to train the model\n","\n","\n","def train():\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","    # empty list to save model predictions\n","    total_preds = []\n","    # iterate over batches\n","    for step, batch in enumerate(train_dataloader):\n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(\n","                step, len(train_dataloader)))\n","        # push the batch to gpu\n","        batch = [r.to(device) for r in batch]\n","        sent_id, mask, labels = batch\n","        # clear previously calculated gradients\n","        model.zero_grad()\n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask)\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","        # add on to the total loss\n","        total_loss = total_loss + loss.item()\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        # update parameters\n","        optimizer.step()\n","        # model predictions are stored on GPU. So, push it to CPU\n","        preds = preds.detach().cpu().numpy()\n","        # append the model predictions\n","        total_preds.append(preds)\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","    # returns the loss and predictions\n","    return avg_loss, total_preds\n","\n","\n","# function for evaluating the model\n","def evaluate():\n","\n","    print(\"\\nEvaluating...\")\n","\n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save the model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    for step, batch in enumerate(val_dataloader):\n","\n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","\n","            # Calculate elapsed time in minutes.\n","            # elapsed = format_time(time.time() - t0)\n","\n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(\n","                step, len(val_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","\n","            # model predictions\n","            preds = model(sent_id, mask)\n","\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds, labels)\n","\n","            total_loss = total_loss + loss.item()\n","\n","            preds = preds.detach().cpu().numpy()\n","\n","            total_preds.append(preds)\n","\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(val_dataloader)\n","\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","\n","    return avg_loss, total_preds\n","\n",""]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Epoch 1 / 10\n","  Batch    50  of    706.\n","  Batch   100  of    706.\n","  Batch   150  of    706.\n","  Batch   200  of    706.\n","  Batch   250  of    706.\n","  Batch   300  of    706.\n","  Batch   350  of    706.\n","  Batch   400  of    706.\n","  Batch   450  of    706.\n","  Batch   500  of    706.\n","  Batch   550  of    706.\n","  Batch   600  of    706.\n","  Batch   650  of    706.\n","  Batch   700  of    706.\n","\n","Evaluating...\n","  Batch    50  of    152.\n","  Batch   100  of    152.\n","  Batch   150  of    152.\n","\n","Training Loss: 0.650\n","Validation Loss: 0.567\n","\n"," Epoch 2 / 10\n","  Batch    50  of    706.\n","  Batch   100  of    706.\n","  Batch   150  of    706.\n","  Batch   200  of    706.\n","  Batch   250  of    706.\n","  Batch   300  of    706.\n","  Batch   350  of    706.\n","  Batch   400  of    706.\n","  Batch   450  of    706.\n","  Batch   500  of    706.\n","  Batch   550  of    706.\n","  Batch   600  of    706.\n","  Batch   650  of    706.\n","  Batch   700  of    706.\n","\n","Evaluating...\n","  Batch    50  of    152.\n","  Batch   100  of    152.\n","  Batch   150  of    152.\n","\n","Training Loss: 0.597\n","Validation Loss: 0.579\n","\n"," Epoch 3 / 10\n","  Batch    50  of    706.\n","  Batch   100  of    706.\n","  Batch   150  of    706.\n","  Batch   200  of    706.\n","  Batch   250  of    706.\n","  Batch   300  of    706.\n","  Batch   350  of    706.\n","  Batch   400  of    706.\n","  Batch   450  of    706.\n","  Batch   500  of    706.\n","  Batch   550  of    706.\n","  Batch   600  of    706.\n","  Batch   650  of    706.\n","  Batch   700  of    706.\n","\n","Evaluating...\n","  Batch    50  of    152.\n","  Batch   100  of    152.\n","  Batch   150  of    152.\n","\n","Training Loss: 0.582\n","Validation Loss: 0.537\n","\n"," Epoch 4 / 10\n","  Batch    50  of    706.\n","  Batch   100  of    706.\n","  Batch   150  of    706.\n","  Batch   200  of    706.\n","  Batch   250  of    706.\n","  Batch   300  of    706.\n","  Batch   350  of    706.\n","  Batch   400  of    706.\n","  Batch   450  of    706.\n","  Batch   500  of    706.\n","  Batch   550  of    706.\n","  Batch   600  of    706.\n","  Batch   650  of    706.\n","  Batch   700  of    706.\n","\n","Evaluating...\n","  Batch    50  of    152.\n","  Batch   100  of    152.\n","  Batch   150  of    152.\n","\n","Training Loss: 0.575\n","Validation Loss: 0.527\n","\n"," Epoch 5 / 10\n","  Batch    50  of    706.\n","  Batch   100  of    706.\n","  Batch   150  of    706.\n","  Batch   200  of    706.\n","  Batch   250  of    706.\n","  Batch   300  of    706.\n","  Batch   350  of    706.\n","  Batch   400  of    706.\n","  Batch   450  of    706.\n","  Batch   500  of    706.\n","  Batch   550  of    706.\n","  Batch   600  of    706.\n","  Batch   650  of    706.\n","  Batch   700  of    706.\n","\n","Evaluating...\n","  Batch    50  of    152.\n","  Batch   100  of    152.\n","  Batch   150  of    152.\n","\n","Training Loss: 0.573\n","Validation Loss: 0.526\n","\n"," Epoch 6 / 10\n","  Batch    50  of    706.\n","  Batch   100  of    706.\n","  Batch   150  of    706.\n","  Batch   200  of    706.\n","  Batch   250  of    706.\n","  Batch   300  of    706.\n","  Batch   350  of    706.\n","  Batch   400  of    706.\n","  Batch   450  of    706.\n","  Batch   500  of    706.\n","  Batch   550  of    706.\n","  Batch   600  of    706.\n","  Batch   650  of    706.\n","  Batch   700  of    706.\n","\n","Evaluating...\n","  Batch    50  of    152.\n","  Batch   100  of    152.\n","  Batch   150  of    152.\n","\n","Training Loss: 0.572\n","Validation Loss: 0.575\n","\n"," Epoch 7 / 10\n","  Batch    50  of    706.\n","  Batch   100  of    706.\n","  Batch   150  of    706.\n","  Batch   200  of    706.\n","  Batch   250  of    706.\n","  Batch   300  of    706.\n","  Batch   350  of    706.\n","  Batch   400  of    706.\n","  Batch   450  of    706.\n","  Batch   500  of    706.\n","  Batch   550  of    706.\n","  Batch   600  of    706.\n","  Batch   650  of    706.\n","  Batch   700  of    706.\n","\n","Evaluating...\n","  Batch    50  of    152.\n","  Batch   100  of    152.\n","  Batch   150  of    152.\n","\n","Training Loss: 0.562\n","Validation Loss: 0.539\n","\n"," Epoch 8 / 10\n","  Batch    50  of    706.\n","  Batch   100  of    706.\n","  Batch   150  of    706.\n","  Batch   200  of    706.\n","  Batch   250  of    706.\n","  Batch   300  of    706.\n","  Batch   350  of    706.\n","  Batch   400  of    706.\n","  Batch   450  of    706.\n","  Batch   500  of    706.\n","  Batch   550  of    706.\n","  Batch   600  of    706.\n","  Batch   650  of    706.\n","  Batch   700  of    706.\n","\n","Evaluating...\n","  Batch    50  of    152.\n","  Batch   100  of    152.\n","  Batch   150  of    152.\n","\n","Training Loss: 0.562\n","Validation Loss: 0.557\n","\n"," Epoch 9 / 10\n","  Batch    50  of    706.\n","  Batch   100  of    706.\n","  Batch   150  of    706.\n","  Batch   200  of    706.\n","  Batch   250  of    706.\n","  Batch   300  of    706.\n","  Batch   350  of    706.\n","  Batch   400  of    706.\n","  Batch   450  of    706.\n","  Batch   500  of    706.\n","  Batch   550  of    706.\n","  Batch   600  of    706.\n","  Batch   650  of    706.\n","  Batch   700  of    706.\n","\n","Evaluating...\n","  Batch    50  of    152.\n","  Batch   100  of    152.\n","  Batch   150  of    152.\n","\n","Training Loss: 0.557\n","Validation Loss: 0.519\n","\n"," Epoch 10 / 10\n","  Batch    50  of    706.\n","  Batch   100  of    706.\n","  Batch   150  of    706.\n","  Batch   200  of    706.\n","  Batch   250  of    706.\n","  Batch   300  of    706.\n","  Batch   350  of    706.\n","  Batch   400  of    706.\n","  Batch   450  of    706.\n","  Batch   500  of    706.\n","  Batch   550  of    706.\n","  Batch   600  of    706.\n","  Batch   650  of    706.\n","  Batch   700  of    706.\n","\n","Evaluating...\n","  Batch    50  of    152.\n","  Batch   100  of    152.\n","  Batch   150  of    152.\n","\n","Training Loss: 0.559\n","Validation Loss: 0.552\n"]}],"source":["\n","# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses = []\n","valid_losses = []\n","\n","# for each epoch\n","for epoch in range(epochs):\n","\n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","\n","    # train model\n","    train_loss, _ = train()\n","\n","    # evaluate model\n","    valid_loss, _ = evaluate()\n","\n","    # save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","\n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')\n",""]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'saved_weights-tweet_sentiment_classification.pt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m~/Documents/python-projects/nlp-transfer-learnin-test/notebook2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load weights of best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'saved_weights-tweet_sentiment_classification.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/anaconda3/envs/venv-tensorflow-pytorch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/venv-tensorflow-pytorch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/venv-tensorflow-pytorch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_weights-tweet_sentiment_classification.pt'"]}],"source":["\n","# load weights of best model\n","path = 'saved_weights-tweet_sentiment_classification.pt'\n","model.load_state_dict(torch.load(path))\n",""]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'saved_weights-tweet_sentiment_classification-27102020.pt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m~/Documents/python-projects/nlp-transfer-learnin-test/notebook2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load weights of best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'saved_weights-tweet_sentiment_classification-27102020.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/anaconda3/envs/venv-tensorflow-pytorch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/venv-tensorflow-pytorch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/venv-tensorflow-pytorch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_weights-tweet_sentiment_classification-27102020.pt'"]}],"source":["\n","# load weights of best model\n","path = 'saved_weights-tweet_sentiment_classification-27102020.pt'\n","model.load_state_dict(torch.load(path))\n",""]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'saved_weights.pt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m~/Documents/python-projects/nlp-transfer-learnin-test/notebook2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load weights of best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'saved_weights.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/anaconda3/envs/venv-tensorflow-pytorch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/venv-tensorflow-pytorch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/venv-tensorflow-pytorch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_weights.pt'"]}],"source":["\n","# load weights of best model\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))\n",""]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","from transformers import AdamW\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from IPython.core.interactiveshell import InteractiveShell\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","import time\n","\n","# specify GPU\n","device = torch.device(\"cuda\")\n","\n","# This is for multiple print statements per cell\n","InteractiveShell.ast_node_interactivity = \"all\"\n",""]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text\n","0  negative                       oh no its fading away again \n","1  positive  bunnylake will kill me but i cant stop listeni...\n","2  negative  last day in cali  partyin for the last time wi...\n","3  negative                     is having a major soar throat \n","4  positive                       my last day as 12 years old "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>oh no its fading away again</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>bunnylake will kill me but i cant stop listeni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>last day in cali  partyin for the last time wi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>negative</td>\n      <td>is having a major soar throat</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>my last day as 12 years old</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":106}],"source":["df = pd.read_csv(\"pre-cleaned_consolidated_tweet_data.csv\", sep='\\t')\n","df.head()\n",""]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["frac = 0.002\n","# sample and shuffle the dataset according to the fraction choise in the line above\n","df = df.sample(frac=frac).reset_index(drop=True)\n","\n",""]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text  label\n","0  positive  i just realized that i aspire to be a cross be...      1\n","1  negative  ohgoodnesspris you named it jonathan junior wo...      0\n","2  negative  really disappointed that shaheen jafargholi di...      0\n","3  positive  i cant believe i have my very own miniature ha...      1\n","4  negative  i love my niece i really do but i want her to ...      0"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>positive</td>\n      <td>i just realized that i aspire to be a cross be...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>negative</td>\n      <td>ohgoodnesspris you named it jonathan junior wo...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>really disappointed that shaheen jafargholi di...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>positive</td>\n      <td>i cant believe i have my very own miniature ha...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>negative</td>\n      <td>i love my niece i really do but i want her to ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":108}],"source":["df['label'] = df.apply(lambda x: 1 if x['sentiment']\n","                       == 'positive' else 0, axis=1)\n","df.head()\n",""]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   label                                               text\n","0      1  i just realized that i aspire to be a cross be...\n","1      0  ohgoodnesspris you named it jonathan junior wo...\n","2      0  really disappointed that shaheen jafargholi di...\n","3      1  i cant believe i have my very own miniature ha...\n","4      0  i love my niece i really do but i want her to ..."],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>i just realized that i aspire to be a cross be...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>ohgoodnesspris you named it jonathan junior wo...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>really disappointed that shaheen jafargholi di...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>i cant believe i have my very own miniature ha...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>i love my niece i really do but i want her to ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":109}],"source":["df = df[['label', 'text']]\n","df.head()\n","\n",""]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[],"source":["train_text, temp_text, train_labels, temp_labels = \\\n","    train_test_split(df['text'], df['label'], random_state=2018,\n","                     test_size=0.3, stratify=df['label'])\n","\n","\n","val_text, test_text, val_labels, test_labels = \\\n","    train_test_split(temp_text, temp_labels, random_state=2018,\n","                     test_size=0.5, stratify=temp_labels)\n","\n",""]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[],"source":["bert = AutoModel.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[],"source":["text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",""]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[],"source":["sent_id = tokenizer.batch_encode_plus(\n","    text, padding=True, return_token_type_ids=False)\n",""]},{"cell_type":"code","execution_count":115,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}],"source":["print(sent_id)\n",""]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AxesSubplot:>"]},"metadata":{},"execution_count":116},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 375.2875 248.518125\" width=\"375.2875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-10-27T23:30:39.959131</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 375.2875 248.518125 \nL 375.2875 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 224.64 \nL 368.0875 224.64 \nL 368.0875 7.2 \nL 33.2875 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 48.505682 224.64 \nL 58.651136 224.64 \nL 58.651136 163.114534 \nL 48.505682 163.114534 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 58.651136 224.64 \nL 68.796591 224.64 \nL 68.796591 124.098385 \nL 58.651136 124.098385 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 68.796591 224.64 \nL 78.942045 224.64 \nL 78.942045 64.07354 \nL 68.796591 64.07354 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 78.942045 224.64 \nL 89.0875 224.64 \nL 89.0875 61.072298 \nL 78.942045 61.072298 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 89.0875 224.64 \nL 99.232955 224.64 \nL 99.232955 17.554286 \nL 89.0875 17.554286 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 99.232955 224.64 \nL 109.378409 224.64 \nL 109.378409 38.562981 \nL 99.232955 38.562981 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 109.378409 224.64 \nL 119.523864 224.64 \nL 119.523864 44.565466 \nL 109.378409 44.565466 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 119.523864 224.64 \nL 129.669318 224.64 \nL 129.669318 64.07354 \nL 119.523864 64.07354 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 129.669318 224.64 \nL 139.814773 224.64 \nL 139.814773 67.074783 \nL 129.669318 67.074783 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 139.814773 224.64 \nL 149.960227 224.64 \nL 149.960227 68.575404 \nL 139.814773 68.575404 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 149.960227 224.64 \nL 160.105682 224.64 \nL 160.105682 64.07354 \nL 149.960227 64.07354 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 160.105682 224.64 \nL 170.251136 224.64 \nL 170.251136 52.068571 \nL 160.105682 52.068571 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 170.251136 224.64 \nL 180.396591 224.64 \nL 180.396591 91.08472 \nL 170.251136 91.08472 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 180.396591 224.64 \nL 190.542045 224.64 \nL 190.542045 83.581615 \nL 180.396591 83.581615 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 190.542045 224.64 \nL 200.6875 224.64 \nL 200.6875 104.590311 \nL 190.542045 104.590311 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 200.6875 224.64 \nL 210.832955 224.64 \nL 210.832955 119.596522 \nL 200.6875 119.596522 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 210.832955 224.64 \nL 220.978409 224.64 \nL 220.978409 103.089689 \nL 210.832955 103.089689 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 220.978409 224.64 \nL 231.123864 224.64 \nL 231.123864 115.094658 \nL 220.978409 115.094658 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 231.123864 224.64 \nL 241.269318 224.64 \nL 241.269318 101.589068 \nL 231.123864 101.589068 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 241.269318 224.64 \nL 251.414773 224.64 \nL 251.414773 128.600248 \nL 241.269318 128.600248 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 251.414773 224.64 \nL 261.560227 224.64 \nL 261.560227 101.589068 \nL 251.414773 101.589068 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 261.560227 224.64 \nL 271.705682 224.64 \nL 271.705682 116.59528 \nL 261.560227 116.59528 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 271.705682 224.64 \nL 281.851136 224.64 \nL 281.851136 121.097143 \nL 271.705682 121.097143 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 281.851136 224.64 \nL 291.996591 224.64 \nL 291.996591 142.105839 \nL 281.851136 142.105839 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 291.996591 224.64 \nL 302.142045 224.64 \nL 302.142045 167.616398 \nL 291.996591 167.616398 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 302.142045 224.64 \nL 312.2875 224.64 \nL 312.2875 193.126957 \nL 302.142045 193.126957 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 312.2875 224.64 \nL 322.432955 224.64 \nL 322.432955 194.627578 \nL 312.2875 194.627578 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 322.432955 224.64 \nL 332.578409 224.64 \nL 332.578409 202.130683 \nL 322.432955 202.130683 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 332.578409 224.64 \nL 342.723864 224.64 \nL 342.723864 218.637516 \nL 332.578409 218.637516 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path clip-path=\"url(#p26c2743203)\" d=\"M 342.723864 224.64 \nL 352.869318 224.64 \nL 352.869318 220.138137 \nL 342.723864 220.138137 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p26c2743203)\" d=\"M 38.6875 224.64 \nL 38.6875 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m74919e9752\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.6875\" xlink:href=\"#m74919e9752\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(35.50625 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p26c2743203)\" d=\"M 87.778409 224.64 \nL 87.778409 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"87.778409\" xlink:href=\"#m74919e9752\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(84.597159 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p26c2743203)\" d=\"M 136.869318 224.64 \nL 136.869318 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"136.869318\" xlink:href=\"#m74919e9752\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(130.506818 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p26c2743203)\" d=\"M 185.960227 224.64 \nL 185.960227 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.960227\" xlink:href=\"#m74919e9752\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(179.597727 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p26c2743203)\" d=\"M 235.051136 224.64 \nL 235.051136 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.051136\" xlink:href=\"#m74919e9752\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(228.688636 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p26c2743203)\" d=\"M 284.142045 224.64 \nL 284.142045 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"284.142045\" xlink:href=\"#m74919e9752\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(277.779545 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p26c2743203)\" d=\"M 333.232955 224.64 \nL 333.232955 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"333.232955\" xlink:href=\"#m74919e9752\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <g transform=\"translate(326.870455 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p26c2743203)\" d=\"M 33.2875 224.64 \nL 368.0875 224.64 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m2df74b4719\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2df74b4719\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 228.439219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p26c2743203)\" d=\"M 33.2875 194.627578 \nL 368.0875 194.627578 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2df74b4719\" y=\"194.627578\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 20 -->\n      <g transform=\"translate(13.5625 198.426796)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p26c2743203)\" d=\"M 33.2875 164.615155 \nL 368.0875 164.615155 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2df74b4719\" y=\"164.615155\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 40 -->\n      <g transform=\"translate(13.5625 168.414374)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p26c2743203)\" d=\"M 33.2875 134.602733 \nL 368.0875 134.602733 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2df74b4719\" y=\"134.602733\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 60 -->\n      <g transform=\"translate(13.5625 138.401952)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p26c2743203)\" d=\"M 33.2875 104.590311 \nL 368.0875 104.590311 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2df74b4719\" y=\"104.590311\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 80 -->\n      <g transform=\"translate(13.5625 108.389529)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p26c2743203)\" d=\"M 33.2875 74.577888 \nL 368.0875 74.577888 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2df74b4719\" y=\"74.577888\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 78.377107)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p26c2743203)\" d=\"M 33.2875 44.565466 \nL 368.0875 44.565466 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2df74b4719\" y=\"44.565466\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 120 -->\n      <g transform=\"translate(7.2 48.364685)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p26c2743203)\" d=\"M 33.2875 14.553043 \nL 368.0875 14.553043 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2df74b4719\" y=\"14.553043\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 140 -->\n      <g transform=\"translate(7.2 18.352262)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 33.2875 224.64 \nL 33.2875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 368.0875 224.64 \nL 368.0875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 33.2875 224.64 \nL 368.0875 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 33.2875 7.2 \nL 368.0875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p26c2743203\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["seq_len = [len(i.split()) for i in train_text]\n","\n","pd.Series(seq_len).hist(bins=30)\n",""]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[],"source":["max_seq_len = 30\n",""]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[],"source":["tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[],"source":["tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":120,"metadata":{},"outputs":[],"source":["tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[],"source":["\n","# for train set\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","# for validation set\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","# for test set\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())\n",""]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[],"source":["\n","# define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(\n","    train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(\n","    val_data, sampler=val_sampler, batch_size=batch_size)\n",""]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[],"source":["\n","# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = False\n",""]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[],"source":["\n","\n","class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","        super(BERT_Arch, self).__init__()\n","        self.bert = bert\n","        # dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","        # relu activation function\n","        self.relu = nn.ReLU()\n","        # dense layer 1\n","        self.fc1 = nn.Linear(768, 512)\n","        # dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512, 2)\n","        # softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    # define the forward pass\n","    def forward(self, sent_id, mask):\n","        # pass the inputs to the model\n","        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n","        x = self.fc1(cls_hs)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        # output layer\n","        x = self.fc2(x)\n","        # apply softmax activation\n","        x = self.softmax(x)\n","        return x\n","\n",""]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[],"source":["model = BERT_Arch(bert)\n",""]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[],"source":["model = model.to(device)\n","\n","# optimizer from hugging face transformers\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr=1e-3)\n",""]},{"cell_type":"code","execution_count":127,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.98086957 1.0198915 ]\n"]}],"source":["\n","\n","# compute the class weights\n","class_wts = compute_class_weight(\n","    'balanced', np.unique(train_labels), train_labels)\n","\n","print(class_wts)\n","\n","# convert class weights to tensor\n","weights = torch.tensor(class_wts, dtype=torch.float)\n","weights = weights.to(device)\n","\n","# loss function\n","cross_entropy = nn.NLLLoss(weight=weights)\n","\n","# number of training epochs\n","epochs = 10\n",""]},{"cell_type":"code","execution_count":128,"metadata":{},"outputs":[],"source":["\n","# function to train the model\n","\n","\n","def train():\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","    # empty list to save model predictions\n","    total_preds = []\n","    # iterate over batches\n","    for step, batch in enumerate(train_dataloader):\n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(\n","                step, len(train_dataloader)))\n","        # push the batch to gpu\n","        batch = [r.to(device) for r in batch]\n","        sent_id, mask, labels = batch\n","        # clear previously calculated gradients\n","        model.zero_grad()\n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask)\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","        # add on to the total loss\n","        total_loss = total_loss + loss.item()\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        # update parameters\n","        optimizer.step()\n","        # model predictions are stored on GPU. So, push it to CPU\n","        preds = preds.detach().cpu().numpy()\n","        # append the model predictions\n","        total_preds.append(preds)\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","    # returns the loss and predictions\n","    return avg_loss, total_preds\n","\n","\n","# function for evaluating the model\n","def evaluate():\n","\n","    print(\"\\nEvaluating...\")\n","\n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save the model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    for step, batch in enumerate(val_dataloader):\n","\n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","\n","            # Calculate elapsed time in minutes.\n","            # elapsed = format_time(time.time() - t0)\n","\n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(\n","                step, len(val_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","\n","            # model predictions\n","            preds = model(sent_id, mask)\n","\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds, labels)\n","\n","            total_loss = total_loss + loss.item()\n","\n","            preds = preds.detach().cpu().numpy()\n","\n","            total_preds.append(preds)\n","\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(val_dataloader)\n","\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","\n","    return avg_loss, total_preds\n","\n",""]},{"cell_type":"code","execution_count":129,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Epoch 1 / 10\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.701\n","Validation Loss: 0.643\n","\n"," Epoch 2 / 10\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.670\n","Validation Loss: 0.629\n","\n"," Epoch 3 / 10\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.654\n","Validation Loss: 0.621\n","\n"," Epoch 4 / 10\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.651\n","Validation Loss: 0.595\n","\n"," Epoch 5 / 10\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.631\n","Validation Loss: 0.622\n","\n"," Epoch 6 / 10\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.628\n","Validation Loss: 0.582\n","\n"," Epoch 7 / 10\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.616\n","Validation Loss: 0.602\n","\n"," Epoch 8 / 10\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.614\n","Validation Loss: 0.563\n","\n"," Epoch 9 / 10\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.586\n","Validation Loss: 0.635\n","\n"," Epoch 10 / 10\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.612\n","Validation Loss: 0.559\n"]}],"source":["\n","# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses = []\n","valid_losses = []\n","\n","# for each epoch\n","for epoch in range(epochs):\n","\n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","\n","    # train model\n","    train_loss, _ = train()\n","\n","    # evaluate model\n","    valid_loss, _ = evaluate()\n","\n","    # save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","\n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')\n",""]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":130}],"source":["\n","# load weights of best model\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))\n",""]},{"cell_type":"code","execution_count":131,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BERT_Arch(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (relu): ReLU()\n","  (fc1): Linear(in_features=768, out_features=512, bias=True)\n","  (fc2): Linear(in_features=512, out_features=2, bias=True)\n","  (softmax): LogSoftmax(dim=1)\n",")"]},"metadata":{},"execution_count":131},{"output_type":"execute_result","data":{"text/plain":["BERT_Arch(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (relu): ReLU()\n","  (fc1): Linear(in_features=768, out_features=512, bias=True)\n","  (fc2): Linear(in_features=512, out_features=2, bias=True)\n","  (softmax): LogSoftmax(dim=1)\n",")"]},"metadata":{},"execution_count":131},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n\n           0       0.81      0.51      0.63       247\n           1       0.63      0.87      0.73       237\n\n    accuracy                           0.69       484\n   macro avg       0.72      0.69      0.68       484\nweighted avg       0.72      0.69      0.68       484\n\n"]},{"output_type":"execute_result","data":{"text/plain":["col_0    0    1\n","row_0          \n","0      126  121\n","1       30  207"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>col_0</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>row_0</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>126</td>\n      <td>121</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>30</td>\n      <td>207</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":131}],"source":["# useful : https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html\n","# Put model in evaluation mode\n","model.cpu()\n","model.eval()\n","# torch.cuda.empty_cache()\n","# get predictions for test data\n","with torch.no_grad():\n","    # test_seq, test_mask =\n","    # preds = model(test_seq.to(device), test_mask.to(device))\n","    preds = model(test_seq, test_mask)\n","\n","    # preds = preds.detach().cpu().numpy()\n","\n","# model's performance\n","preds = np.argmax(preds, axis=1)\n","print(classification_report(test_y, preds))\n","\n","# confusion matrix\n","pd.crosstab(test_y, preds)\n",""]},{"cell_type":"code","execution_count":132,"metadata":{},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","from transformers import AdamW\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from IPython.core.interactiveshell import InteractiveShell\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","import time\n","\n","# specify GPU\n","device = torch.device(\"cuda\")\n","\n","# This is for multiple print statements per cell\n","InteractiveShell.ast_node_interactivity = \"all\"\n",""]},{"cell_type":"code","execution_count":133,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text\n","0  negative                       oh no its fading away again \n","1  positive  bunnylake will kill me but i cant stop listeni...\n","2  negative  last day in cali  partyin for the last time wi...\n","3  negative                     is having a major soar throat \n","4  positive                       my last day as 12 years old "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>oh no its fading away again</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>bunnylake will kill me but i cant stop listeni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>last day in cali  partyin for the last time wi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>negative</td>\n      <td>is having a major soar throat</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>my last day as 12 years old</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":133}],"source":["df = pd.read_csv(\"pre-cleaned_consolidated_tweet_data.csv\", sep='\\t')\n","df.head()\n",""]},{"cell_type":"code","execution_count":134,"metadata":{},"outputs":[],"source":["frac = 0.002\n","# sample and shuffle the dataset according to the fraction choise in the line above\n","df = df.sample(frac=frac).reset_index(drop=True)\n","\n",""]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text  label\n","0  positive                              sat at youthy club  x      1\n","1  positive  minor mechanical failure  thought id snapped b...      1\n","2  positive  lemlemishere im drinking mezcal  i still have ...      1\n","3  negative                    man im gonna miss cruz so much       0\n","4  positive  eating tic tacs on the train i think one just ...      1"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>positive</td>\n      <td>sat at youthy club  x</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>minor mechanical failure  thought id snapped b...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>positive</td>\n      <td>lemlemishere im drinking mezcal  i still have ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>negative</td>\n      <td>man im gonna miss cruz so much</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>eating tic tacs on the train i think one just ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":135}],"source":["df['label'] = df.apply(lambda x: 1 if x['sentiment']\n","                       == 'positive' else 0, axis=1)\n","df.head()\n",""]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   label                                               text\n","0      1                              sat at youthy club  x\n","1      1  minor mechanical failure  thought id snapped b...\n","2      1  lemlemishere im drinking mezcal  i still have ...\n","3      0                    man im gonna miss cruz so much \n","4      1  eating tic tacs on the train i think one just ..."],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>sat at youthy club  x</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>minor mechanical failure  thought id snapped b...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>lemlemishere im drinking mezcal  i still have ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>man im gonna miss cruz so much</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>eating tic tacs on the train i think one just ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":136}],"source":["df = df[['label', 'text']]\n","df.head()\n","\n",""]},{"cell_type":"code","execution_count":137,"metadata":{},"outputs":[],"source":["train_text, temp_text, train_labels, temp_labels = \\\n","    train_test_split(df['text'], df['label'], random_state=2018,\n","                     test_size=0.3, stratify=df['label'])\n","\n","\n","val_text, test_text, val_labels, test_labels = \\\n","    train_test_split(temp_text, temp_labels, random_state=2018,\n","                     test_size=0.5, stratify=temp_labels)\n","\n",""]},{"cell_type":"code","execution_count":138,"metadata":{},"outputs":[],"source":["bert = AutoModel.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":139,"metadata":{},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[],"source":["text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",""]},{"cell_type":"code","execution_count":141,"metadata":{},"outputs":[],"source":["sent_id = tokenizer.batch_encode_plus(\n","    text, padding=True, return_token_type_ids=False)\n",""]},{"cell_type":"code","execution_count":142,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}],"source":["print(sent_id)\n",""]},{"cell_type":"code","execution_count":143,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AxesSubplot:>"]},"metadata":{},"execution_count":143},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 375.2875 248.518125\" width=\"375.2875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-10-27T23:36:59.116334</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 375.2875 248.518125 \nL 375.2875 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 224.64 \nL 368.0875 224.64 \nL 368.0875 7.2 \nL 33.2875 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 48.505682 224.64 \nL 58.651136 224.64 \nL 58.651136 161.32717 \nL 48.505682 161.32717 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 58.651136 224.64 \nL 68.796591 224.64 \nL 68.796591 132.30879 \nL 58.651136 132.30879 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 68.796591 224.64 \nL 78.942045 224.64 \nL 78.942045 88.781219 \nL 68.796591 88.781219 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 78.942045 224.64 \nL 89.0875 224.64 \nL 89.0875 76.910064 \nL 78.942045 76.910064 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 89.0875 224.64 \nL 99.232955 224.64 \nL 99.232955 59.762839 \nL 89.0875 59.762839 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 99.232955 224.64 \nL 109.378409 224.64 \nL 109.378409 50.529718 \nL 99.232955 50.529718 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 109.378409 224.64 \nL 119.523864 224.64 \nL 119.523864 51.848735 \nL 109.378409 51.848735 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 119.523864 224.64 \nL 129.669318 224.64 \nL 129.669318 51.848735 \nL 119.523864 51.848735 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 129.669318 224.64 \nL 139.814773 224.64 \nL 139.814773 57.124804 \nL 129.669318 57.124804 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 139.814773 224.64 \nL 149.960227 224.64 \nL 149.960227 95.376306 \nL 139.814773 95.376306 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 149.960227 224.64 \nL 160.105682 224.64 \nL 160.105682 82.186133 \nL 149.960227 82.186133 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 160.105682 224.64 \nL 170.251136 224.64 \nL 170.251136 95.376306 \nL 160.105682 95.376306 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 170.251136 224.64 \nL 180.396591 224.64 \nL 180.396591 104.609427 \nL 170.251136 104.609427 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 180.396591 224.64 \nL 190.542045 224.64 \nL 190.542045 88.781219 \nL 180.396591 88.781219 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 190.542045 224.64 \nL 200.6875 224.64 \nL 200.6875 128.351738 \nL 190.542045 128.351738 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 200.6875 224.64 \nL 210.832955 224.64 \nL 210.832955 17.554286 \nL 200.6875 17.554286 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 210.832955 224.64 \nL 220.978409 224.64 \nL 220.978409 125.713703 \nL 210.832955 125.713703 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 220.978409 224.64 \nL 231.123864 224.64 \nL 231.123864 117.7996 \nL 220.978409 117.7996 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 231.123864 224.64 \nL 241.269318 224.64 \nL 241.269318 150.775032 \nL 231.123864 150.775032 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 241.269318 224.64 \nL 251.414773 224.64 \nL 251.414773 129.670755 \nL 241.269318 129.670755 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 251.414773 224.64 \nL 261.560227 224.64 \nL 261.560227 149.456015 \nL 251.414773 149.456015 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 261.560227 224.64 \nL 271.705682 224.64 \nL 271.705682 146.81798 \nL 261.560227 146.81798 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 271.705682 224.64 \nL 281.851136 224.64 \nL 281.851136 162.646187 \nL 271.705682 162.646187 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 281.851136 224.64 \nL 291.996591 224.64 \nL 291.996591 183.750464 \nL 281.851136 183.750464 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 291.996591 224.64 \nL 302.142045 224.64 \nL 302.142045 196.940637 \nL 291.996591 196.940637 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 302.142045 224.64 \nL 312.2875 224.64 \nL 312.2875 187.707516 \nL 302.142045 187.707516 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 312.2875 224.64 \nL 322.432955 224.64 \nL 322.432955 204.854741 \nL 312.2875 204.854741 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 322.432955 224.64 \nL 332.578409 224.64 \nL 332.578409 220.682948 \nL 322.432955 220.682948 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 332.578409 224.64 \nL 342.723864 224.64 \nL 342.723864 224.64 \nL 332.578409 224.64 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path clip-path=\"url(#p439fe18694)\" d=\"M 342.723864 224.64 \nL 352.869318 224.64 \nL 352.869318 219.363931 \nL 342.723864 219.363931 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 38.994318 224.64 \nL 38.994318 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mc03869d25b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.994318\" xlink:href=\"#mc03869d25b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(35.813068 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 86.551136 224.64 \nL 86.551136 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"86.551136\" xlink:href=\"#mc03869d25b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(83.369886 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 134.107955 224.64 \nL 134.107955 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"134.107955\" xlink:href=\"#mc03869d25b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(127.745455 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 181.664773 224.64 \nL 181.664773 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"181.664773\" xlink:href=\"#mc03869d25b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(175.302273 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 229.221591 224.64 \nL 229.221591 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"229.221591\" xlink:href=\"#mc03869d25b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(222.859091 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 276.778409 224.64 \nL 276.778409 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"276.778409\" xlink:href=\"#mc03869d25b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(270.415909 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 324.335227 224.64 \nL 324.335227 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.335227\" xlink:href=\"#mc03869d25b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <g transform=\"translate(317.972727 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 33.2875 224.64 \nL 368.0875 224.64 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m19b3c41eb0\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m19b3c41eb0\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 228.439219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 33.2875 198.259654 \nL 368.0875 198.259654 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m19b3c41eb0\" y=\"198.259654\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 20 -->\n      <g transform=\"translate(13.5625 202.058873)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 33.2875 171.879308 \nL 368.0875 171.879308 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m19b3c41eb0\" y=\"171.879308\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 40 -->\n      <g transform=\"translate(13.5625 175.678527)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 33.2875 145.498963 \nL 368.0875 145.498963 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m19b3c41eb0\" y=\"145.498963\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 60 -->\n      <g transform=\"translate(13.5625 149.298181)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 33.2875 119.118617 \nL 368.0875 119.118617 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m19b3c41eb0\" y=\"119.118617\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 80 -->\n      <g transform=\"translate(13.5625 122.917836)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 33.2875 92.738271 \nL 368.0875 92.738271 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m19b3c41eb0\" y=\"92.738271\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 96.53749)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 33.2875 66.357925 \nL 368.0875 66.357925 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m19b3c41eb0\" y=\"66.357925\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 120 -->\n      <g transform=\"translate(7.2 70.157144)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 33.2875 39.97758 \nL 368.0875 39.97758 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m19b3c41eb0\" y=\"39.97758\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 140 -->\n      <g transform=\"translate(7.2 43.776798)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#p439fe18694)\" d=\"M 33.2875 13.597234 \nL 368.0875 13.597234 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m19b3c41eb0\" y=\"13.597234\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 160 -->\n      <g transform=\"translate(7.2 17.396453)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 33.2875 224.64 \nL 33.2875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 368.0875 224.64 \nL 368.0875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 33.2875 224.64 \nL 368.0875 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 33.2875 7.2 \nL 368.0875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p439fe18694\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["seq_len = [len(i.split()) for i in train_text]\n","\n","pd.Series(seq_len).hist(bins=30)\n",""]},{"cell_type":"code","execution_count":144,"metadata":{},"outputs":[],"source":["max_seq_len = 30\n",""]},{"cell_type":"code","execution_count":145,"metadata":{},"outputs":[],"source":["tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":146,"metadata":{},"outputs":[],"source":["tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":147,"metadata":{},"outputs":[],"source":["tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":148,"metadata":{},"outputs":[],"source":["\n","# for train set\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","# for validation set\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","# for test set\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())\n",""]},{"cell_type":"code","execution_count":149,"metadata":{},"outputs":[],"source":["\n","# define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(\n","    train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(\n","    val_data, sampler=val_sampler, batch_size=batch_size)\n",""]},{"cell_type":"code","execution_count":150,"metadata":{},"outputs":[],"source":["\n","# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = False\n",""]},{"cell_type":"code","execution_count":151,"metadata":{},"outputs":[],"source":["\n","\n","class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","        super(BERT_Arch, self).__init__()\n","        self.bert = bert\n","        # dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","        # relu activation function\n","        self.relu = nn.ReLU()\n","        # dense layer 1\n","        self.fc1 = nn.Linear(768, 512)\n","        # dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512, 2)\n","        # softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    # define the forward pass\n","    def forward(self, sent_id, mask):\n","        # pass the inputs to the model\n","        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n","        x = self.fc1(cls_hs)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        # output layer\n","        x = self.fc2(x)\n","        # apply softmax activation\n","        x = self.softmax(x)\n","        return x\n","\n",""]},{"cell_type":"code","execution_count":152,"metadata":{},"outputs":[],"source":["model = BERT_Arch(bert)\n",""]},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[],"source":["model = model.to(device)\n","\n","# optimizer from hugging face transformers\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr=1e-3)\n",""]},{"cell_type":"code","execution_count":154,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["[1.0080429  0.99208443]\n"]}],"source":["\n","\n","# compute the class weights\n","class_wts = compute_class_weight(\n","    'balanced', np.unique(train_labels), train_labels)\n","\n","print(class_wts)\n","\n","# convert class weights to tensor\n","weights = torch.tensor(class_wts, dtype=torch.float)\n","weights = weights.to(device)\n","\n","# loss function\n","cross_entropy = nn.NLLLoss(weight=weights)\n","\n","# number of training epochs\n","epochs = 20\n",""]},{"cell_type":"code","execution_count":155,"metadata":{},"outputs":[],"source":["\n","# function to train the model\n","\n","\n","def train():\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","    # empty list to save model predictions\n","    total_preds = []\n","    # iterate over batches\n","    for step, batch in enumerate(train_dataloader):\n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(\n","                step, len(train_dataloader)))\n","        # push the batch to gpu\n","        batch = [r.to(device) for r in batch]\n","        sent_id, mask, labels = batch\n","        # clear previously calculated gradients\n","        model.zero_grad()\n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask)\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","        # add on to the total loss\n","        total_loss = total_loss + loss.item()\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        # update parameters\n","        optimizer.step()\n","        # model predictions are stored on GPU. So, push it to CPU\n","        preds = preds.detach().cpu().numpy()\n","        # append the model predictions\n","        total_preds.append(preds)\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","    # returns the loss and predictions\n","    return avg_loss, total_preds\n","\n","\n","# function for evaluating the model\n","def evaluate():\n","\n","    print(\"\\nEvaluating...\")\n","\n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save the model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    for step, batch in enumerate(val_dataloader):\n","\n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","\n","            # Calculate elapsed time in minutes.\n","            # elapsed = format_time(time.time() - t0)\n","\n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(\n","                step, len(val_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","\n","            # model predictions\n","            preds = model(sent_id, mask)\n","\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds, labels)\n","\n","            total_loss = total_loss + loss.item()\n","\n","            preds = preds.detach().cpu().numpy()\n","\n","            total_preds.append(preds)\n","\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(val_dataloader)\n","\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","\n","    return avg_loss, total_preds\n","\n",""]},{"cell_type":"code","execution_count":156,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Epoch 1 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.708\n","Validation Loss: 0.676\n","\n"," Epoch 2 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.681\n","Validation Loss: 0.648\n","\n"," Epoch 3 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.640\n","Validation Loss: 0.666\n","\n"," Epoch 4 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.626\n","Validation Loss: 0.611\n","\n"," Epoch 5 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.614\n","Validation Loss: 0.634\n","\n"," Epoch 6 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.603\n","Validation Loss: 0.576\n","\n"," Epoch 7 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.611\n","Validation Loss: 0.570\n","\n"," Epoch 8 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.617\n","Validation Loss: 0.582\n","\n"," Epoch 9 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.617\n","Validation Loss: 0.690\n","\n"," Epoch 10 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.613\n","Validation Loss: 0.612\n","\n"," Epoch 11 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.601\n","Validation Loss: 0.572\n","\n"," Epoch 12 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.586\n","Validation Loss: 0.556\n","\n"," Epoch 13 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.602\n","Validation Loss: 0.586\n","\n"," Epoch 14 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.590\n","Validation Loss: 0.575\n","\n"," Epoch 15 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.598\n","Validation Loss: 0.594\n","\n"," Epoch 16 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.576\n","Validation Loss: 0.542\n","\n"," Epoch 17 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.574\n","Validation Loss: 0.643\n","\n"," Epoch 18 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.568\n","Validation Loss: 0.562\n","\n"," Epoch 19 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.580\n","Validation Loss: 0.557\n","\n"," Epoch 20 / 20\n","  Batch    50  of     71.\n","\n","Evaluating...\n","\n","Training Loss: 0.562\n","Validation Loss: 0.551\n"]}],"source":["\n","# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses = []\n","valid_losses = []\n","\n","# for each epoch\n","for epoch in range(epochs):\n","\n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","\n","    # train model\n","    train_loss, _ = train()\n","\n","    # evaluate model\n","    valid_loss, _ = evaluate()\n","\n","    # save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","\n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')\n",""]},{"cell_type":"code","execution_count":157,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":157}],"source":["\n","# load weights of best model\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))\n",""]},{"cell_type":"code","execution_count":158,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BERT_Arch(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (relu): ReLU()\n","  (fc1): Linear(in_features=768, out_features=512, bias=True)\n","  (fc2): Linear(in_features=512, out_features=2, bias=True)\n","  (softmax): LogSoftmax(dim=1)\n",")"]},"metadata":{},"execution_count":158},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n\n           0       0.71      0.66      0.69       240\n           1       0.69      0.73      0.71       244\n\n    accuracy                           0.70       484\n   macro avg       0.70      0.70      0.70       484\nweighted avg       0.70      0.70      0.70       484\n\n"]},{"output_type":"execute_result","data":{"text/plain":["col_0    0    1\n","row_0          \n","0      159   81\n","1       65  179"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>col_0</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>row_0</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>159</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>65</td>\n      <td>179</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":158}],"source":["# useful : https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html\n","# Put model in evaluation mode\n","model.cpu()\n","# model.eval()\n","# torch.cuda.empty_cache()\n","# get predictions for test data\n","with torch.no_grad():\n","    # test_seq, test_mask =\n","    # preds = model(test_seq.to(device), test_mask.to(device))\n","    preds = model(test_seq, test_mask)\n","\n","    # preds = preds.detach().cpu().numpy()\n","\n","# model's performance\n","preds = np.argmax(preds, axis=1)\n","print(classification_report(test_y, preds))\n","\n","# confusion matrix\n","pd.crosstab(test_y, preds)\n",""]},{"cell_type":"code","execution_count":159,"metadata":{},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","from transformers import AdamW\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from IPython.core.interactiveshell import InteractiveShell\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","import time\n","\n","# specify GPU\n","device = torch.device(\"cuda\")\n","\n","# This is for multiple print statements per cell\n","InteractiveShell.ast_node_interactivity = \"all\"\n",""]},{"cell_type":"code","execution_count":160,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text\n","0  negative                       oh no its fading away again \n","1  positive  bunnylake will kill me but i cant stop listeni...\n","2  negative  last day in cali  partyin for the last time wi...\n","3  negative                     is having a major soar throat \n","4  positive                       my last day as 12 years old "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>oh no its fading away again</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>bunnylake will kill me but i cant stop listeni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>last day in cali  partyin for the last time wi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>negative</td>\n      <td>is having a major soar throat</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>my last day as 12 years old</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":160}],"source":["df = pd.read_csv(\"pre-cleaned_consolidated_tweet_data.csv\", sep='\\t')\n","df.head()\n",""]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[],"source":["frac = 0.01\n","# sample and shuffle the dataset according to the fraction choise in the line above\n","df = df.sample(frac=frac).reset_index(drop=True)\n","\n",""]},{"cell_type":"code","execution_count":162,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  sentiment                                               text  label\n","0  positive  jeanlogan but is he a twitter expert i think not       1\n","1  negative     feet are killing off new shoes for sixth form       0\n","2  positive  on my way homme  goooood show tonight lt3 haha...      1\n","3  positive  httptwitpiccom6rrh2  the family  matt son adam...      1\n","4  positive  exams soon s  oh well  watchin old school  goo...      1"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>positive</td>\n      <td>jeanlogan but is he a twitter expert i think not</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>negative</td>\n      <td>feet are killing off new shoes for sixth form</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>positive</td>\n      <td>on my way homme  goooood show tonight lt3 haha...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>positive</td>\n      <td>httptwitpiccom6rrh2  the family  matt son adam...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>exams soon s  oh well  watchin old school  goo...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":162}],"source":["df['label'] = df.apply(lambda x: 1 if x['sentiment']\n","                       == 'positive' else 0, axis=1)\n","df.head()\n",""]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   label                                               text\n","0      1  jeanlogan but is he a twitter expert i think not \n","1      0     feet are killing off new shoes for sixth form \n","2      1  on my way homme  goooood show tonight lt3 haha...\n","3      1  httptwitpiccom6rrh2  the family  matt son adam...\n","4      1  exams soon s  oh well  watchin old school  goo..."],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>jeanlogan but is he a twitter expert i think not</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>feet are killing off new shoes for sixth form</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>on my way homme  goooood show tonight lt3 haha...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>httptwitpiccom6rrh2  the family  matt son adam...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>exams soon s  oh well  watchin old school  goo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":163}],"source":["df = df[['label', 'text']]\n","df.head()\n","\n",""]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[],"source":["train_text, temp_text, train_labels, temp_labels = \\\n","    train_test_split(df['text'], df['label'], random_state=2018,\n","                     test_size=0.3, stratify=df['label'])\n","\n","\n","val_text, test_text, val_labels, test_labels = \\\n","    train_test_split(temp_text, temp_labels, random_state=2018,\n","                     test_size=0.5, stratify=temp_labels)\n","\n",""]},{"cell_type":"code","execution_count":165,"metadata":{},"outputs":[],"source":["bert = AutoModel.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":166,"metadata":{},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",""]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[],"source":["text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",""]},{"cell_type":"code","execution_count":168,"metadata":{},"outputs":[],"source":["sent_id = tokenizer.batch_encode_plus(\n","    text, padding=True, return_token_type_ids=False)\n",""]},{"cell_type":"code","execution_count":169,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}],"source":["print(sent_id)\n",""]},{"cell_type":"code","execution_count":170,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AxesSubplot:>"]},"metadata":{},"execution_count":170},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 375.2875 248.518125\" width=\"375.2875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-10-27T23:47:53.484841</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 375.2875 248.518125 \nL 375.2875 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 224.64 \nL 368.0875 224.64 \nL 368.0875 7.2 \nL 33.2875 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 48.505682 224.64 \nL 58.651136 224.64 \nL 58.651136 157.422952 \nL 48.505682 157.422952 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 58.651136 224.64 \nL 68.796591 224.64 \nL 68.796591 114.518453 \nL 58.651136 114.518453 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 68.796591 224.64 \nL 78.942045 224.64 \nL 78.942045 91.922084 \nL 68.796591 91.922084 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 78.942045 224.64 \nL 89.0875 224.64 \nL 89.0875 65.607324 \nL 78.942045 65.607324 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 89.0875 224.64 \nL 99.232955 224.64 \nL 99.232955 40.150655 \nL 89.0875 40.150655 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 99.232955 224.64 \nL 109.378409 224.64 \nL 109.378409 39.006535 \nL 99.232955 39.006535 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 109.378409 224.64 \nL 119.523864 224.64 \nL 119.523864 55.310245 \nL 109.378409 55.310245 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 119.523864 224.64 \nL 129.669318 224.64 \nL 129.669318 46.157285 \nL 119.523864 46.157285 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 129.669318 224.64 \nL 139.814773 224.64 \nL 139.814773 54.452155 \nL 129.669318 54.452155 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 139.814773 224.64 \nL 149.960227 224.64 \nL 149.960227 61.030845 \nL 139.814773 61.030845 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 149.960227 224.64 \nL 160.105682 224.64 \nL 160.105682 74.188224 \nL 149.960227 74.188224 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 160.105682 224.64 \nL 170.251136 224.64 \nL 170.251136 69.611744 \nL 160.105682 69.611744 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 170.251136 224.64 \nL 180.396591 224.64 \nL 180.396591 87.631634 \nL 170.251136 87.631634 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 180.396591 224.64 \nL 190.542045 224.64 \nL 190.542045 92.208114 \nL 180.396591 92.208114 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 190.542045 224.64 \nL 200.6875 224.64 \nL 200.6875 107.367703 \nL 190.542045 107.367703 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 200.6875 224.64 \nL 210.832955 224.64 \nL 210.832955 17.554286 \nL 200.6875 17.554286 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 210.832955 224.64 \nL 220.978409 224.64 \nL 220.978409 118.236843 \nL 210.832955 118.236843 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 220.978409 224.64 \nL 231.123864 224.64 \nL 231.123864 126.817743 \nL 220.978409 126.817743 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 231.123864 224.64 \nL 241.269318 224.64 \nL 241.269318 119.953023 \nL 231.123864 119.953023 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 241.269318 224.64 \nL 251.414773 224.64 \nL 251.414773 120.811113 \nL 241.269318 120.811113 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 251.414773 224.64 \nL 261.560227 224.64 \nL 261.560227 133.682463 \nL 251.414773 133.682463 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 261.560227 224.64 \nL 271.705682 224.64 \nL 271.705682 137.686882 \nL 261.560227 137.686882 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 271.705682 224.64 \nL 281.851136 224.64 \nL 281.851136 157.136922 \nL 271.705682 157.136922 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 281.851136 224.64 \nL 291.996591 224.64 \nL 291.996591 164.287672 \nL 281.851136 164.287672 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 291.996591 224.64 \nL 302.142045 224.64 \nL 302.142045 183.451681 \nL 291.996591 183.451681 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 302.142045 224.64 \nL 312.2875 224.64 \nL 312.2875 200.041421 \nL 302.142045 200.041421 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 312.2875 224.64 \nL 322.432955 224.64 \nL 322.432955 212.62674 \nL 312.2875 212.62674 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 322.432955 224.64 \nL 332.578409 224.64 \nL 332.578409 218.06131 \nL 322.432955 218.06131 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 332.578409 224.64 \nL 342.723864 224.64 \nL 342.723864 222.63779 \nL 332.578409 222.63779 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path clip-path=\"url(#p04d11a9b96)\" d=\"M 342.723864 224.64 \nL 352.869318 224.64 \nL 352.869318 223.20985 \nL 342.723864 223.20985 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p04d11a9b96)\" d=\"M 38.994318 224.64 \nL 38.994318 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m0235b077d6\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.994318\" xlink:href=\"#m0235b077d6\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(35.813068 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p04d11a9b96)\" d=\"M 86.551136 224.64 \nL 86.551136 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"86.551136\" xlink:href=\"#m0235b077d6\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(83.369886 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p04d11a9b96)\" d=\"M 134.107955 224.64 \nL 134.107955 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"134.107955\" xlink:href=\"#m0235b077d6\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(127.745455 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p04d11a9b96)\" d=\"M 181.664773 224.64 \nL 181.664773 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"181.664773\" xlink:href=\"#m0235b077d6\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(175.302273 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p04d11a9b96)\" d=\"M 229.221591 224.64 \nL 229.221591 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"229.221591\" xlink:href=\"#m0235b077d6\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(222.859091 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p04d11a9b96)\" d=\"M 276.778409 224.64 \nL 276.778409 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"276.778409\" xlink:href=\"#m0235b077d6\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(270.415909 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p04d11a9b96)\" d=\"M 324.335227 224.64 \nL 324.335227 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.335227\" xlink:href=\"#m0235b077d6\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <g transform=\"translate(317.972727 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p04d11a9b96)\" d=\"M 33.2875 224.64 \nL 368.0875 224.64 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mccf5547651\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mccf5547651\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 228.439219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p04d11a9b96)\" d=\"M 33.2875 196.037001 \nL 368.0875 196.037001 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mccf5547651\" y=\"196.037001\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 199.83622)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p04d11a9b96)\" d=\"M 33.2875 167.434002 \nL 368.0875 167.434002 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mccf5547651\" y=\"167.434002\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 200 -->\n      <g transform=\"translate(7.2 171.23322)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p04d11a9b96)\" d=\"M 33.2875 138.831002 \nL 368.0875 138.831002 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mccf5547651\" y=\"138.831002\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 300 -->\n      <g transform=\"translate(7.2 142.630221)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p04d11a9b96)\" d=\"M 33.2875 110.228003 \nL 368.0875 110.228003 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mccf5547651\" y=\"110.228003\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 400 -->\n      <g transform=\"translate(7.2 114.027222)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p04d11a9b96)\" d=\"M 33.2875 81.625004 \nL 368.0875 81.625004 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mccf5547651\" y=\"81.625004\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 500 -->\n      <g transform=\"translate(7.2 85.424223)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p04d11a9b96)\" d=\"M 33.2875 53.022005 \nL 368.0875 53.022005 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mccf5547651\" y=\"53.022005\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 600 -->\n      <g transform=\"translate(7.2 56.821223)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p04d11a9b96)\" d=\"M 33.2875 24.419006 \nL 368.0875 24.419006 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mccf5547651\" y=\"24.419006\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 700 -->\n      <g transform=\"translate(7.2 28.218224)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 33.2875 224.64 \nL 33.2875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 368.0875 224.64 \nL 368.0875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 33.2875 224.64 \nL 368.0875 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 33.2875 7.2 \nL 368.0875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p04d11a9b96\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["seq_len = [len(i.split()) for i in train_text]\n","\n","pd.Series(seq_len).hist(bins=30)\n",""]},{"cell_type":"code","execution_count":171,"metadata":{},"outputs":[],"source":["max_seq_len = 30\n",""]},{"cell_type":"code","execution_count":172,"metadata":{},"outputs":[],"source":["tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":173,"metadata":{},"outputs":[],"source":["tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":174,"metadata":{},"outputs":[],"source":["tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length=max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n",""]},{"cell_type":"code","execution_count":175,"metadata":{},"outputs":[],"source":["\n","# for train set\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","# for validation set\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","# for test set\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())\n",""]},{"cell_type":"code","execution_count":176,"metadata":{},"outputs":[],"source":["\n","# define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(\n","    train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(\n","    val_data, sampler=val_sampler, batch_size=batch_size)\n",""]},{"cell_type":"code","execution_count":177,"metadata":{},"outputs":[],"source":["\n","# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = False\n",""]},{"cell_type":"code","execution_count":178,"metadata":{},"outputs":[],"source":["\n","\n","class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","        super(BERT_Arch, self).__init__()\n","        self.bert = bert\n","        # dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","        # relu activation function\n","        self.relu = nn.ReLU()\n","        # dense layer 1\n","        self.fc1 = nn.Linear(768, 512)\n","        # dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512, 2)\n","        # softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    # define the forward pass\n","    def forward(self, sent_id, mask):\n","        # pass the inputs to the model\n","        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n","        x = self.fc1(cls_hs)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        # output layer\n","        x = self.fc2(x)\n","        # apply softmax activation\n","        x = self.softmax(x)\n","        return x\n","\n",""]},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[],"source":["model = BERT_Arch(bert)\n",""]},{"cell_type":"code","execution_count":180,"metadata":{},"outputs":[],"source":["model = model.to(device)\n","\n","# optimizer from hugging face transformers\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr=1e-3)\n",""]},{"cell_type":"code","execution_count":181,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.99734748 1.00266667]\n"]}],"source":["\n","\n","# compute the class weights\n","class_wts = compute_class_weight(\n","    'balanced', np.unique(train_labels), train_labels)\n","\n","print(class_wts)\n","\n","# convert class weights to tensor\n","weights = torch.tensor(class_wts, dtype=torch.float)\n","weights = weights.to(device)\n","\n","# loss function\n","cross_entropy = nn.NLLLoss(weight=weights)\n","\n","# number of training epochs\n","epochs = 20\n",""]},{"cell_type":"code","execution_count":182,"metadata":{},"outputs":[],"source":["\n","# function to train the model\n","\n","\n","def train():\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","    # empty list to save model predictions\n","    total_preds = []\n","    # iterate over batches\n","    for step, batch in enumerate(train_dataloader):\n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(\n","                step, len(train_dataloader)))\n","        # push the batch to gpu\n","        batch = [r.to(device) for r in batch]\n","        sent_id, mask, labels = batch\n","        # clear previously calculated gradients\n","        model.zero_grad()\n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask)\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","        # add on to the total loss\n","        total_loss = total_loss + loss.item()\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        # update parameters\n","        optimizer.step()\n","        # model predictions are stored on GPU. So, push it to CPU\n","        preds = preds.detach().cpu().numpy()\n","        # append the model predictions\n","        total_preds.append(preds)\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","    # returns the loss and predictions\n","    return avg_loss, total_preds\n","\n","\n","# function for evaluating the model\n","def evaluate():\n","\n","    print(\"\\nEvaluating...\")\n","\n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save the model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    for step, batch in enumerate(val_dataloader):\n","\n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","\n","            # Calculate elapsed time in minutes.\n","            # elapsed = format_time(time.time() - t0)\n","\n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(\n","                step, len(val_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","\n","            # model predictions\n","            preds = model(sent_id, mask)\n","\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds, labels)\n","\n","            total_loss = total_loss + loss.item()\n","\n","            preds = preds.detach().cpu().numpy()\n","\n","            total_preds.append(preds)\n","\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(val_dataloader)\n","\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","\n","    return avg_loss, total_preds\n","\n",""]},{"cell_type":"code","execution_count":183,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Epoch 1 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.660\n","Validation Loss: 0.580\n","\n"," Epoch 2 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.627\n","Validation Loss: 0.555\n","\n"," Epoch 3 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.601\n","Validation Loss: 0.558\n","\n"," Epoch 4 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.595\n","Validation Loss: 0.558\n","\n"," Epoch 5 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.585\n","Validation Loss: 0.540\n","\n"," Epoch 6 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.575\n","Validation Loss: 0.554\n","\n"," Epoch 7 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.575\n","Validation Loss: 0.525\n","\n"," Epoch 8 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.576\n","Validation Loss: 0.523\n","\n"," Epoch 9 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.567\n","Validation Loss: 0.592\n","\n"," Epoch 10 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.570\n","Validation Loss: 0.528\n","\n"," Epoch 11 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.576\n","Validation Loss: 0.526\n","\n"," Epoch 12 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.564\n","Validation Loss: 0.555\n","\n"," Epoch 13 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.569\n","Validation Loss: 0.529\n","\n"," Epoch 14 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.567\n","Validation Loss: 0.518\n","\n"," Epoch 15 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.556\n","Validation Loss: 0.515\n","\n"," Epoch 16 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.557\n","Validation Loss: 0.521\n","\n"," Epoch 17 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.557\n","Validation Loss: 0.508\n","\n"," Epoch 18 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.557\n","Validation Loss: 0.527\n","\n"," Epoch 19 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.550\n","Validation Loss: 0.524\n","\n"," Epoch 20 / 20\n","  Batch    50  of    353.\n","  Batch   100  of    353.\n","  Batch   150  of    353.\n","  Batch   200  of    353.\n","  Batch   250  of    353.\n","  Batch   300  of    353.\n","  Batch   350  of    353.\n","\n","Evaluating...\n","  Batch    50  of     76.\n","\n","Training Loss: 0.550\n","Validation Loss: 0.562\n"]}],"source":["\n","# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses = []\n","valid_losses = []\n","\n","# for each epoch\n","for epoch in range(epochs):\n","\n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","\n","    # train model\n","    train_loss, _ = train()\n","\n","    # evaluate model\n","    valid_loss, _ = evaluate()\n","\n","    # save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","\n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')\n",""]},{"cell_type":"code","execution_count":184,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":184}],"source":["\n","# load weights of best model\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))\n",""]},{"cell_type":"code","execution_count":185,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BERT_Arch(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (relu): ReLU()\n","  (fc1): Linear(in_features=768, out_features=512, bias=True)\n","  (fc2): Linear(in_features=512, out_features=2, bias=True)\n","  (softmax): LogSoftmax(dim=1)\n",")"]},"metadata":{},"execution_count":185},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n\n           0       0.77      0.75      0.76      1212\n           1       0.75      0.78      0.76      1206\n\n    accuracy                           0.76      2418\n   macro avg       0.76      0.76      0.76      2418\nweighted avg       0.76      0.76      0.76      2418\n\n"]},{"output_type":"execute_result","data":{"text/plain":["col_0    0    1\n","row_0          \n","0      903  309\n","1      270  936"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>col_0</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>row_0</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>903</td>\n      <td>309</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>270</td>\n      <td>936</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":185}],"source":["# useful : https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html\n","# Put model in evaluation mode\n","model.cpu()\n","# model.eval()\n","# torch.cuda.empty_cache()\n","# get predictions for test data\n","with torch.no_grad():\n","    # test_seq, test_mask =\n","    # preds = model(test_seq.to(device), test_mask.to(device))\n","    preds = model(test_seq, test_mask)\n","\n","    # preds = preds.detach().cpu().numpy()\n","\n","# model's performance\n","preds = np.argmax(preds, axis=1)\n","print(classification_report(test_y, preds))\n","\n","# confusion matrix\n","pd.crosstab(test_y, preds)\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}